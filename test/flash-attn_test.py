import torch
import time
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoModel, AutoConfig
import os
import logging

# é…ç½®æ—¥å¿—ï¼Œæ•è·transformerså†…éƒ¨ä¿¡æ¯
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ¨¡å‹è·¯å¾„
qwen_path = "../models/Qwen/Qwen3-Embedding-0.6B"   # æ›¿æ¢ä¸ºå®é™…è·¯å¾„
bge_path = "D:/å­¦ä¹ èµ„æ–™/æ¯•ä¸šè®¾è®¡/KubAge/models/BAAI/bge-large-zh-v1___5"
qwen_path = bge_path

def verify_flash_attention():
    print("=" * 80)
    print("ğŸ” FLASH ATTENTION éªŒè¯æµç¨‹")
    print("=" * 80)

    # ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒåŸºç¡€æ£€æŸ¥
    print("\n1ï¸âƒ£ ç¯å¢ƒåŸºç¡€æ£€æŸ¥")
    print(f"  â€¢ PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"  â€¢ æ˜¯å¦ä½¿ç”¨C11ABIç¼–è¯‘: {torch.compiled_with_cxx11_abi()}")
    print(f"  â€¢ CUDA å¯ç”¨: {torch.cuda.is_available()}")
    print(f"  â€¢ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"  â€¢ GPU å‹å·: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
    print(f"  â€¢ GPU å†…å­˜: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB")
    print(f"  â€¢ GPU æ¶æ„: {torch.cuda.get_device_capability() if torch.cuda.is_available() else 'N/A'}")
    print(torch.cuda.get_device_properties(0))

    # æ£€æŸ¥ flash-attn æ˜¯å¦å®‰è£…
    try:
        import flash_attn
        print(f"  â€¢ FlashAttention ç‰ˆæœ¬: {flash_attn.__version__}")
    except ImportError as e:
        print(f"  âŒ FlashAttention æœªå®‰è£…: {str(e)}")
        return False

    # ç¬¬äºŒæ­¥ï¼šæ£€æŸ¥æ¨¡å‹é…ç½®
    print("\n2ï¸âƒ£ æ¨¡å‹é…ç½®æ£€æŸ¥")
    try:
        config = AutoConfig.from_pretrained(qwen_path, trust_remote_code=True)
        print(f"  â€¢ æ¨¡å‹æ¶æ„: {config.model_type}")

        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ Flash Attention 2
        supports_flash2 = getattr(config, "_supports_flash_attn_2", False)
        print(f"  â€¢ å£°æ˜æ”¯æŒ Flash Attention 2: {'âœ… æ˜¯' if supports_flash2 else 'âŒ å¦'}")

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å« flash_attn ç›¸å…³ä»£ç 
        flash_files = [f for f in os.listdir(qwen_path) if "flash" in f.lower()]
        print(f"  â€¢ æ¨¡å‹ç›®å½•ä¸­åŒ…å« Flash ç›¸å…³æ–‡ä»¶: {'âœ… ' + str(flash_files) if flash_files else 'âŒ æ— '}")

    except Exception as e:
        print(f"  âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {str(e)}")

    return True

    # ç¬¬ä¸‰æ­¥ï¼šå°è¯•åŠ è½½å¯ç”¨ Flash Attention çš„æ¨¡å‹
    print("\n3ï¸âƒ£ å°è¯•åŠ è½½å¯ç”¨ Flash Attention çš„æ¨¡å‹")
    try:
        # åˆ›å»ºå¯ç”¨ Flash Attention çš„ embeddings
        embeddings_flash = HuggingFaceEmbeddings(
            model_name=qwen_path,
            model_kwargs={
                "device": "cuda",
                "trust_remote_code": True,
                "use_flash_attention_2": True  # å…³é”®å‚æ•°
            },
            encode_kwargs={"normalize_embeddings": True}
        )
        print("  âœ… æˆåŠŸåŠ è½½å¯ç”¨ Flash Attention çš„æ¨¡å‹")

        # è·å–åº•å±‚æ¨¡å‹
        base_model = embeddings_flash.client._model
        print(f"  â€¢ åº•å±‚æ¨¡å‹ç±»å‹: {type(base_model)}")

        # æ£€æŸ¥æ³¨æ„åŠ›å±‚ç±»å‹
        attention_layers = []
        for name, module in base_model.named_modules():
            if "attention" in name.lower():
                attention_layers.append((name, type(module).__name__))

        print(f"  â€¢ æ£€æµ‹åˆ° {len(attention_layers)} ä¸ªæ³¨æ„åŠ›å±‚")
        for name, layer_type in attention_layers[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"    - {name}: {layer_type}")

        # ç‰¹åˆ«æ£€æŸ¥æ˜¯å¦åŒ…å« FlashAttention
        has_flash = any("FlashSelfAttention" in str(type(module))
                        for _, module in base_model.named_modules())
        print(f"  â€¢ åŒ…å« FlashAttention å±‚: {'âœ… æ˜¯' if has_flash else 'âŒ å¦'}")

    except Exception as e:
        print(f"  âŒ åŠ è½½å¤±è´¥: {str(e)}")
        if "does not support Flash Attention 2" in str(e):
            print("    âš ï¸  æ¨¡å‹æ¶æ„ä¸æ”¯æŒ Flash Attention 2")
        return False

    # ç¬¬å››æ­¥ï¼šè¿è¡Œæ—¶éªŒè¯ï¼ˆå®é™…å‰å‘ä¼ æ’­ï¼‰
    print("\n4ï¸âƒ£ è¿è¡Œæ—¶éªŒè¯ (å®é™…æ¨ç†)")
    try:
        # åˆ›å»ºæµ‹è¯•æ–‡æœ¬
        test_texts = ["è¿™æ˜¯ä¸€ä¸ªç”¨äºéªŒè¯Flash Attentionçš„æµ‹è¯•æ–‡æœ¬ã€‚"] * 8  # æ‰¹é‡æµ‹è¯•

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

        # å¯ç”¨Flash Attentionçš„æ¨ç†
        start_time = time.time()
        flash_embeddings = embeddings_flash.embed_documents(test_texts)
        flash_time = time.time() - start_time
        flash_mem = torch.cuda.max_memory_allocated() / 1024 ** 3  # GB

        print(f"  âœ… Flash Attention æ¨ç†æˆåŠŸ!")
        print(f"    â€¢ è€—æ—¶: {flash_time:.4f} ç§’")
        print(f"    â€¢ å³°å€¼å†…å­˜: {flash_mem:.2f} GB")
        print(f"    â€¢ è¾“å‡ºç»´åº¦: {len(flash_embeddings[0])}")

    except Exception as e:
        print(f"  âŒ æ¨ç†å¤±è´¥: {str(e)}")
        return False

    # ç¬¬äº”æ­¥ï¼šå¯¹æ¯”éªŒè¯ï¼ˆç¦ç”¨Flash Attentionï¼‰
    print("\n5ï¸âƒ£ å¯¹æ¯”éªŒè¯ (ç¦ç”¨Flash Attention)")
    try:
        # åˆ›å»ºç¦ç”¨Flash Attentionçš„embeddings
        embeddings_normal = HuggingFaceEmbeddings(
            model_name=qwen_path,
            model_kwargs={
                "device": "cuda",
                "trust_remote_code": True,
                # ä¸å¯ç”¨ Flash Attention
            },
            encode_kwargs={"normalize_embeddings": True}
        )

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

        # ç¦ç”¨Flash Attentionçš„æ¨ç†
        start_time = time.time()
        normal_embeddings = embeddings_normal.embed_documents(test_texts)
        normal_time = time.time() - start_time
        normal_mem = torch.cuda.max_memory_allocated() / 1024 ** 3  # GB

        print(f"  âœ… æ ‡å‡†æ³¨æ„åŠ›æ¨ç†æˆåŠŸ!")
        print(f"    â€¢ è€—æ—¶: {normal_time:.4f} ç§’")
        print(f"    â€¢ å³°å€¼å†…å­˜: {normal_mem:.2f} GB")

        # æ€§èƒ½å¯¹æ¯”
        speedup = normal_time / flash_time if flash_time > 0 else 0
        mem_saving = (normal_mem - flash_mem) / normal_mem * 100 if normal_mem > 0 else 0

        print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
        print(f"    â€¢ é€Ÿåº¦æå‡: {speedup:.2f}x ({normal_time:.4f}s â†’ {flash_time:.4f}s)")
        print(f"    â€¢ å†…å­˜èŠ‚çœ: {mem_saving:.1f}% ({normal_mem:.2f}GB â†’ {flash_mem:.2f}GB)")

        # éªŒè¯è¾“å‡ºä¸€è‡´æ€§
        import numpy as np
        flash_arr = np.array(flash_embeddings)
        normal_arr = np.array(normal_embeddings)
        cos_sim = np.mean(np.sum(flash_arr * normal_arr, axis=1) /
                          (np.linalg.norm(flash_arr, axis=1) * np.linalg.norm(normal_arr, axis=1)))

        print(f"    â€¢ è¾“å‡ºä¸€è‡´æ€§ (ä½™å¼¦ç›¸ä¼¼åº¦): {cos_sim:.6f}")
        if cos_sim > 0.999:
            print("    âœ… è¾“å‡ºé«˜åº¦ä¸€è‡´ï¼ŒéªŒè¯æœ‰æ•ˆ")
        else:
            print("    âš ï¸  è¾“å‡ºå·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½éªŒè¯ä¸å‡†ç¡®")

        # åˆ¤æ–­æ˜¯å¦çœŸæ­£ä½¿ç”¨äº†Flash Attention
        if speedup > 1.2 and mem_saving > 10 and cos_sim > 0.99:
            print("\nğŸ‰ éªŒè¯ç»“è®º: æ¨¡å‹æˆåŠŸä½¿ç”¨äº† Flash Attention!")
            return True
        else:
            print("\nâŒ éªŒè¯ç»“è®º: æœªæ£€æµ‹åˆ° Flash Attention çš„å®é™…æ•ˆæœ")
            print("  å¯èƒ½åŸå› :")
            print("  1. åµŒå…¥æ¨¡å‹åªä½¿ç”¨æµ…å±‚ï¼Œæœªè§¦å‘å®Œæ•´æ³¨æ„åŠ›è®¡ç®—")
            print("  2. è¾“å…¥åºåˆ—å¤ªçŸ­ï¼ŒFlash Attention ä¼˜åŠ¿ä¸æ˜æ˜¾")
            print("  3. æ¨¡å‹æ¶æ„ä¸å®Œå…¨æ”¯æŒ Flash Attention 2")
            return False

    except Exception as e:
        print(f"  âŒ å¯¹æ¯”éªŒè¯å¤±è´¥: {str(e)}")
        return False


# æ‰§è¡ŒéªŒè¯
if __name__ == "__main__":
    result = verify_flash_attention()
    print("\n" + "=" * 80)
    print(f"æœ€ç»ˆéªŒè¯ç»“æœ: {'âœ… æˆåŠŸ' if result else 'âŒ å¤±è´¥'}")
    print("=" * 80)