from typing import List, Dict, Any

from langchain_core.runnables import RunnableConfig
# å¼•å…¥ Embedding ä¾èµ–
from langgraph.constants import START, END
from langgraph.graph import StateGraph

from agent.nodes import RerankNode, RetrievalNode
from agent.schemas import ProblemAnalysis, ExecutionPlan, PlanAction
# å¼•å…¥é¡¹ç›®æ¨¡å— (è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´)
from agent.state import AgentState
from retriever import MilvusHybridRetriever
from test_dataset.retrieval_cases import ALL_RETRIEVAL_SCENARIOS, RetrievalTestScenario
from utils import get_dense_embed_model, get_sparse_embed_model
from utils.milvus_adapter import connect_milvus_by_env


# ==========================================
# 1. å®šä¹‰ Dummy Analysis Node (è™šæ‹ŸèŠ‚ç‚¹)
# ==========================================
class DummyAnalysisNode:
    """
    ä¸€ä¸ªä¼ªé€ çš„åˆ†æèŠ‚ç‚¹ï¼Œå®ƒä¸è°ƒç”¨ LLMï¼Œ
    è€Œæ˜¯ç›´æ¥ä»è¾“å…¥çš„ input['analysis'] ä¸­è¯»å–é¢„è®¾å¥½çš„ Analysis å¯¹è±¡ï¼Œ
    å¹¶æ›´æ–°åˆ° State ä¸­ã€‚
    """

    def __call__(self, state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
        print("\n--- [Dummy Analysis Node] Injecting Mock Data ---")
        # è¿™é‡Œçš„ state åœ¨ invoke æ—¶ä¼šä¼ å…¥æˆ‘ä»¬æ„é€ çš„åˆå§‹æ•°æ®
        # æˆ‘ä»¬çº¦å®šåœ¨ metadata ä¸­ä¼ å…¥é¢„è®¾çš„ analysis å¯¹è±¡
        metadata = state.get("metadata", {})
        mock_analysis: ProblemAnalysis = metadata.get("inject_analysis")
        queries = metadata.get("inject_plan_queries")
        exePlan = ExecutionPlan(
            reasoning="Dummy Reason",
            action=PlanAction.RETRIEVE,
            search_queries=queries,
        )

        if not mock_analysis:
            raise ValueError("Test Error: No mock analysis data found in metadata!")

        print(f"âœ… Injected Analysis for operation: {mock_analysis.target_operation}")
        return {"analysis": mock_analysis, "plan": exePlan}


# ==========================================
# 2. æµ‹è¯•ä¸»æµç¨‹
# ==========================================
def retrieval_workflow_test(scenarios: List[RetrievalTestScenario]):
    print("ğŸš€ Starting Retrieval Node Workflow Test Batch...")
    # --- A. è¿æ¥ Milvus
    connect_milvus_by_env()

    # --- B. åˆå§‹åŒ–èµ„æº (ä¸€æ¬¡æ€§åŠ è½½æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½) ---
    print("â³ Initializing Embeddings and Retriever (this may take a while)...")

    # è¯·æ ¹æ®ä½ çš„å®é™…æ¨¡å‹è·¯å¾„ä¿®æ”¹
    DENSE_MODEL_PATH = "../models/Qwen/Qwen3-Embedding-0.6B"
    SPARSE_MODEL_PATH = "BAAI/bge-m3"
    RERANKER_MODEL_PATH = "../models/Qwen/Qwen3-Reranker-0.6B"
    COLLECTION_NAME = "knowledge_base_v2"

    # 1. Dense Embedding
    dense_embedding = get_dense_embed_model(DENSE_MODEL_PATH)

    # 2. Sparse Embedding
    sparse_embedding = get_sparse_embed_model(SPARSE_MODEL_PATH)

    # 3. Retriever
    retriever = MilvusHybridRetriever(
        collection_name=COLLECTION_NAME,
        dense_embedding_func=dense_embedding,
        sparse_embedding_func=sparse_embedding,
        top_k=5
    )

    # --- B. æ„å»ºå›¾ (Build the Graph) ---
    workflow = StateGraph(AgentState)

    dummy_analysis_node = DummyAnalysisNode()
    reranker = RerankNode(
        model_path=RERANKER_MODEL_PATH,
        top_n=3,
    )
    retrieval_node = RetrievalNode(retriever, reranker)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("mock_analysis", dummy_analysis_node)
    workflow.add_node("retrieve_docs", retrieval_node)

    # å®šä¹‰è¾¹
    workflow.add_edge(START, "mock_analysis")
    workflow.add_edge("mock_analysis", "retrieve_docs")
    workflow.add_edge("retrieve_docs", END)

    app = workflow.compile()

    # --- C. å¾ªç¯è¿è¡Œæµ‹è¯•ç”¨ä¾‹ ---
    success_count = 0
    total_count = len(scenarios)

    for i, case in enumerate(scenarios, 1):
        print(f"\n{'=' * 20} Test Case {i}/{total_count}: {case.name} {'=' * 20}")
        if case.description:
            print(f"ğŸ“ Description: {case.description}")

        try:
            # æ„é€ è¾“å…¥
            # æˆ‘ä»¬é€šè¿‡ metadata æŠŠ mock_analysis ä¼ é€’ç»™ DummyNode
            inputs = {
                "messages": [],  # æ£€ç´¢èŠ‚ç‚¹å…¶å®ä¸çœ‹ messagesï¼Œåªçœ‹ analysis
                "metadata": {
                    "inject_analysis": case.mock_analysis,
                    "inject_plan_queries": case.mock_plan_queries
                }
            }

            print(f"â³ Invoking Workflow...")
            final_state = app.invoke(inputs)

            # è·å–ç»“æœ
            retrieved_docs = final_state.get("retrieved_docs", [])

            # æ‰“å°éƒ¨åˆ†ç»“æœç”¨äºäººå·¥æ£€æŸ¥
            print(f"\nğŸ“„ Final Retrieved {len(retrieved_docs)} documents.")
            for idx, doc in enumerate(retrieved_docs):  # åªæ‰“å°å‰3æ¡é¿å…åˆ·å±
                score = doc.metadata.get('rerank_score', 'N/A')
                print(f"   [Doc {idx + 1}] Source: {doc.metadata.get('source', 'unknown')}")
                print(f"   Title: {doc.metadata.get('title')}")
                print(f"   Snippet: {doc.page_content[:50].replace('\n', ' ')}...")
                print(f"   Score: {score}")

            # æ‰§è¡ŒéªŒè¯
            print("ğŸ” Verifying results...")
            case.verify_func(retrieved_docs)

            print(f"âœ… Passed!")
            success_count += 1

        except Exception as e:
            print(f"âŒ Test Failed: {str(e)}")
            import traceback
            traceback.print_exc()

    print("\n" + "=" * 60)
    print(f"ğŸ“Š Retrieval Test Summary: {success_count}/{total_count} passed.")
    print("=" * 60)


if __name__ == "__main__":
    retrieval_workflow_test(ALL_RETRIEVAL_SCENARIOS)