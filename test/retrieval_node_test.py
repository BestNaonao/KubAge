import os
from typing import List, Dict, Any

import torch
from dotenv import load_dotenv, find_dotenv
from langchain_core.runnables import RunnableConfig
# å¼•å…¥ Embedding ä¾èµ–
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.constants import START, END
from langgraph.graph import StateGraph
from pymilvus import connections
from pymilvus.model.hybrid import BGEM3EmbeddingFunction

from agent.nodes.rerank_node import RerankNode
from agent.nodes.retrieval_node import RetrievalNode
# å¼•å…¥é¡¹ç›®æ¨¡å— (è¯·æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´)
from agent.state import AgentState
from retriever.MilvusHybridRetriever import MilvusHybridRetriever
from test_dataset.retrieval_cases import ALL_RETRIEVAL_SCENARIOS, RetrievalTestScenario


# ==========================================
# 1. å®šä¹‰ Dummy Analysis Node (è™šæ‹ŸèŠ‚ç‚¹)
# ==========================================
class DummyAnalysisNode:
    """
    ä¸€ä¸ªä¼ªé€ çš„åˆ†æèŠ‚ç‚¹ï¼Œå®ƒä¸è°ƒç”¨ LLMï¼Œ
    è€Œæ˜¯ç›´æ¥ä»è¾“å…¥çš„ input['analysis'] ä¸­è¯»å–é¢„è®¾å¥½çš„ Analysis å¯¹è±¡ï¼Œ
    å¹¶æ›´æ–°åˆ° State ä¸­ã€‚
    """

    def __call__(self, state: dict, config: RunnableConfig) -> Dict[str, Any]:
        print("\n--- [Dummy Analysis Node] Injecting Mock Data ---")
        # è¿™é‡Œçš„ state åœ¨ invoke æ—¶ä¼šä¼ å…¥æˆ‘ä»¬æ„é€ çš„åˆå§‹æ•°æ®
        # æˆ‘ä»¬çº¦å®šåœ¨ metadata ä¸­ä¼ å…¥é¢„è®¾çš„ analysis å¯¹è±¡
        metadata = state.get("metadata", {})
        mock_analysis = metadata.get("inject_analysis")

        if not mock_analysis:
            raise ValueError("Test Error: No mock analysis data found in metadata!")

        print(f"âœ… Injected Analysis for operation: {mock_analysis.target_operation}")
        return {"analysis": mock_analysis}


# ==========================================
# 2. æµ‹è¯•ä¸»æµç¨‹
# ==========================================
def retrieval_workflow_test(scenarios: List[RetrievalTestScenario]):
    print("ğŸš€ Starting Retrieval Node Workflow Test Batch...")
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(find_dotenv())
    host = os.getenv('MILVUS_HOST', 'localhost')
    port = os.getenv('MILVUS_PORT', '19530')
    user = os.getenv('MILVUS_USER', 'root')
    password = os.getenv('MILVUS_ROOT_PASSWORD', 'Milvus')

    # --- A. è¿æ¥ Milvus
    print(f"æ­£åœ¨è¿æ¥ Milvus ({host}:{port})...")
    connections.connect(alias="default", host=host, port=port, user=user, password=password)

    # --- B. åˆå§‹åŒ–èµ„æº (ä¸€æ¬¡æ€§åŠ è½½æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½) ---
    print("â³ Initializing Embeddings and Retriever (this may take a while)...")

    # è¯·æ ¹æ®ä½ çš„å®é™…æ¨¡å‹è·¯å¾„ä¿®æ”¹
    DENSE_MODEL_PATH = "../models/Qwen/Qwen3-Embedding-0.6B"
    SPARSE_MODEL_PATH = "BAAI/bge-m3"
    RERANKER_MODEL_PATH = "../models/Qwen/Qwen3-Reranker-0.6B"
    COLLECTION_NAME = "knowledge_base_v2"

    # 1. Dense Embedding
    dense_embedding = HuggingFaceEmbeddings(
        model_name=DENSE_MODEL_PATH,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )

    # 2. Sparse Embedding
    sparse_embedding = BGEM3EmbeddingFunction(
        model_name=SPARSE_MODEL_PATH,
        use_fp16=True,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # 3. Retriever
    retriever = MilvusHybridRetriever(
        collection_name=COLLECTION_NAME,
        dense_embedding_func=dense_embedding,
        sparse_embedding_func=sparse_embedding,
        top_k=5
    )

    # --- B. æ„å»ºå›¾ (Build the Graph) ---
    workflow = StateGraph(AgentState)

    dummy_analysis_node = DummyAnalysisNode()
    retrieval_node = RetrievalNode(retriever)
    rerank_node = RerankNode(
        model_path=RERANKER_MODEL_PATH,
        top_n=3,
    )

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("mock_analysis", dummy_analysis_node)
    workflow.add_node("retrieve_docs", retrieval_node)
    workflow.add_node("rerank_docs", rerank_node)

    # å®šä¹‰è¾¹
    workflow.add_edge(START, "mock_analysis")
    workflow.add_edge("mock_analysis", "retrieve_docs")
    workflow.add_edge("retrieve_docs", "rerank_docs")
    workflow.add_edge("rerank_docs", END)

    app = workflow.compile()

    # --- C. å¾ªç¯è¿è¡Œæµ‹è¯•ç”¨ä¾‹ ---
    success_count = 0
    total_count = len(scenarios)

    for i, case in enumerate(scenarios, 1):
        print(f"\n{'=' * 20} Test Case {i}/{total_count}: {case.name} {'=' * 20}")
        if case.description:
            print(f"ğŸ“ Description: {case.description}")

        try:
            # æ„é€ è¾“å…¥
            # æˆ‘ä»¬é€šè¿‡ metadata æŠŠ mock_analysis ä¼ é€’ç»™ DummyNode
            inputs = {
                "messages": [],  # æ£€ç´¢èŠ‚ç‚¹å…¶å®ä¸çœ‹ messagesï¼Œåªçœ‹ analysis
                "metadata": {
                    "inject_analysis": case.mock_analysis
                }
            }

            print(f"â³ Invoking Workflow...")
            final_state = app.invoke(inputs)

            # è·å–ç»“æœ
            retrieved_chunks = final_state.get("retrieved_chunks", [])

            # æ‰“å°éƒ¨åˆ†ç»“æœç”¨äºäººå·¥æ£€æŸ¥
            print(f"\nğŸ“„ Final Retrieved {len(retrieved_chunks)} documents.")
            for idx, doc in enumerate(retrieved_chunks):  # åªæ‰“å°å‰3æ¡é¿å…åˆ·å±
                score = doc.metadata.get('rerank_score', 'N/A')
                print(f"   [Doc {idx + 1}] Source: {doc.metadata.get('source', 'unknown')}")
                print(f"   Title: {doc.metadata.get('title')}")
                print(f"   Snippet: {doc.page_content[:50].replace('\n', ' ')}...")
                print(f"   Score: {score}")

            # æ‰§è¡ŒéªŒè¯
            print("ğŸ” Verifying results...")
            case.verify_func(retrieved_chunks)

            print(f"âœ… Passed!")
            success_count += 1

        except Exception as e:
            print(f"âŒ Test Failed: {str(e)}")
            import traceback
            traceback.print_exc()

    print("\n" + "=" * 60)
    print(f"ğŸ“Š Retrieval Test Summary: {success_count}/{total_count} passed.")
    print("=" * 60)


if __name__ == "__main__":
    retrieval_workflow_test(ALL_RETRIEVAL_SCENARIOS)