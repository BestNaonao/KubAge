import logging
from typing import List, Set, Any

import numpy as np
from langchain_core.documents import Document
from pymilvus import Collection

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class GraphTraverser:
    """
    å›¾æ‹“æ‰‘æ‰©å±•å™¨
    è´Ÿè´£åŸºäºåˆå§‹é”šç‚¹æ–‡æ¡£ (Anchors) è¿›è¡Œçˆ¶çº§é€’å½’æ‰©å±•å’Œé“¾æ¥æ‹“æ‰‘æ‰©å±•
    """

    def __init__(
            self,
            milvus_collection_name: str,
            milvus_connection_alias: str = "default",
            parent_decay_threshold: float = 0.75,
            absolute_min_similarity: float = 0.2,  # é˜²æ­¢ç›¸å…³æ€§å¤ªä½
            link_proportion: float = 0.75,
            max_link_top_k: int = 5
    ):
        self.collection_name = milvus_collection_name
        self.alias = milvus_connection_alias

        # é˜ˆå€¼é…ç½®
        self.decay_threshold = parent_decay_threshold
        self.min_sim = absolute_min_similarity
        self.link_proportion = link_proportion
        self.max_link_top_k = max_link_top_k

    def expand(self, anchors: List[Document], query_vec: List[float]) -> List[Document]:
        """
        æ‰§è¡Œå›¾æ‰©å±•çš„ä¸»å…¥å£
        Args:
            anchors: åˆå§‹æ£€ç´¢åˆ°çš„é”šç‚¹æ–‡æ¡£åˆ—è¡¨
            query_vec: é¢„è®¡ç®—å¥½çš„ Query ç¨ å¯†å‘é‡ (æ¥è‡ª RetrievalNode çš„ç¼“å­˜)
        """
        if not anchors:
            return []

        # å»ºç«‹å·²å­˜åœ¨ ID é›†åˆï¼Œç”¨äºå»é‡
        existing_pks = {doc.metadata.get("pk") for doc in anchors if doc.metadata.get("pk")}

        # 1. çˆ¶çº§é€’å½’æ‰©å±•
        parent_docs = self._expand_parents(anchors, query_vec, existing_pks)
        print(f"   â¬†ï¸  Parent Expansion: Found {len(parent_docs)} docs")

        # 2. é“¾æ¥æ‹“æ‰‘æ‰©å±•
        link_docs = self._expand_links(anchors, query_vec, existing_pks)
        print(f"   ğŸ”— Link Expansion: Found {len(link_docs)} docs")

        # 3. åˆå¹¶ç»“æœ (æ­¤æ—¶æ‰€æœ‰æ–‡æ¡£å·²å»é‡ä¸”æ ‡è®°äº† metadata)
        # æ³¨æ„ï¼šè¿™é‡Œåªè¿”å›æ–°å¢çš„æ‰©å±•æ–‡æ¡£ï¼Œè¿˜æ˜¯è¿”å›å…¨éƒ¨ï¼Ÿ
        # é€šå¸¸ Traverser è¿”å›æ‰©å±•éƒ¨åˆ†ï¼Œç”±è°ƒç”¨æ–¹åˆå¹¶ã€‚ä½†ä¸ºäº†æ–¹ä¾¿ï¼Œè¿™é‡Œè¿”å› List[ExpandedDoc]
        return parent_docs + link_docs

    def _expand_parents(self, anchors: List[Document], query_vec: List[float], existing_pks: Set[str]) -> List[Document]:
        """
        å‘ä¸Šé€’å½’æ‰©å±•çˆ¶èŠ‚ç‚¹
        é€»è¾‘ï¼šSim_j >= Sim_child * Threshold
        """
        expanded_docs = []

        # é˜Ÿåˆ—ï¼šå­˜å‚¨ (doc_id, child_similarity_score, source_anchor_text)
        # åˆå§‹é˜¶æ®µï¼Œæˆ‘ä»¬å°† anchor è§†ä¸º "child"ï¼Œå…¶ similarity è®¾ä¸º 1.0 (æˆ–è€…åŸºäºæ£€ç´¢åˆ†ï¼Œè¿™é‡Œç®€åŒ–ä¸º 1.0 ä½œä¸ºåŸºå‡†)
        # æˆ–è€…æ›´ä¸¥æ ¼ï¼šè®¡ç®— Anchor æœ¬èº«ä¸ Query çš„ç›¸ä¼¼åº¦ä½œä¸ºåŸºå‡†

        next_batch_ids = []
        # è®°å½•æ¯ä¸ªçˆ¶IDå¯¹åº”çš„åŸºå‡†åˆ†æ•°å’Œæ¥æºé”šç‚¹
        # Map: parent_id -> (child_score, anchor_text)
        candidates_map = {}

        # --- åˆå§‹åŒ–ï¼šä» Anchors è·å–ç¬¬ä¸€å±‚ Parent ---
        for doc in anchors:
            parent_id = doc.metadata.get("parent_id")
            if parent_id and parent_id not in existing_pks:
                # è®¡ç®—å½“å‰ Anchor çš„ç›¸ä¼¼åº¦ä½œä¸ºåŸºå‡† Sim_child
                # å¦‚æœæ²¡æœ‰å‘é‡ï¼Œæš‚æ—¶ç”¨ 1.0ï¼Œä½†åœ¨ä¸¥æ ¼æ¨¡å¼ä¸‹åº”è¯¥è®¡ç®—
                sim_child = 1.0
                if "summary_vector" in doc.metadata and doc.metadata["summary_vector"]:
                    sim_child = self._cosine_sim(query_vec, doc.metadata["vector"])

                # å¦‚æœå¤šä¸ªå­èŠ‚ç‚¹æŒ‡å‘åŒä¸€ä¸ªçˆ¶èŠ‚ç‚¹ï¼Œå–åˆ†æ•°æœ€é«˜çš„é‚£ä¸ªè·¯å¾„
                if parent_id not in candidates_map or sim_child > candidates_map[parent_id][0]:
                    candidates_map[parent_id] = (sim_child, doc.page_content[:50])
                    next_batch_ids.append(parent_id)

        # --- é€’å½’å¾ªç¯ ---
        # è®¾ç½®æœ€å¤§æ·±åº¦é˜²æ­¢æ­»å¾ªç¯ï¼Œä¾‹å¦‚ 5 å±‚
        depth = 0
        while next_batch_ids and depth < 5:
            depth += 1
            # 1. æ‰¹é‡æ‹‰å–çˆ¶æ–‡æ¡£ (åŒ…å« summary_vector)
            fetched_docs = self._batch_fetch(next_batch_ids)

            current_generation_ids = []

            for doc in fetched_docs:
                pk = doc.metadata.get("pk")

                # è·å–è¯¥æ–‡æ¡£çš„ "å­èŠ‚ç‚¹åˆ†æ•°" å’Œ "æ¥æº"
                child_score, source_text = candidates_map.get(pk, (0.0, ""))

                # 2. è®¡ç®—å½“å‰çˆ¶èŠ‚ç‚¹çš„ç›¸ä¼¼åº¦ Sim_j (ä½¿ç”¨ summary_vector)
                summary_vec = doc.metadata.get("summary_vector")
                if not summary_vec:
                    continue  # æ²¡æœ‰å‘é‡æ— æ³•è®¡ç®—ï¼Œè·³è¿‡

                sim_j = self._cosine_sim(query_vec, summary_vec)

                # 3. é˜ˆå€¼åˆ¤æ–­é€»è¾‘
                # è§„åˆ™: Sim_j >= Sim_child * Threshold
                # åŒæ—¶ä¹Ÿå¿…é¡»æ»¡è¶³ç»å¯¹åº•çº¿ min_sim
                required_score = child_score * self.decay_threshold

                if sim_j >= required_score and sim_j > self.min_sim:
                    # --> æ¥å—è¯¥çˆ¶èŠ‚ç‚¹
                    doc.metadata["expansion_type"] = "parent"
                    doc.metadata["expansion_source"] = f"Parent of anchor: '{source_text}...'"
                    doc.metadata["expansion_score"] = float(sim_j)

                    # åŠ å…¥ç»“æœé›†
                    if pk not in existing_pks:
                        existing_pks.add(pk)
                        expanded_docs.append(doc)

                        # 4. å‡†å¤‡ä¸‹ä¸€è½®é€’å½’ï¼šè·å–è¯¥èŠ‚ç‚¹çš„ parent
                        grand_parent_id = doc.metadata.get("parent_id")
                        if grand_parent_id and grand_parent_id not in existing_pks:
                            # è®°å½•å½“å‰èŠ‚ç‚¹çš„åˆ†æ•°ï¼Œä½œä¸ºä¸‹ä¸€çº§çš„ "child_score"
                            if grand_parent_id not in candidates_map or sim_j > candidates_map[grand_parent_id][0]:
                                candidates_map[grand_parent_id] = (sim_j, source_text)
                                current_generation_ids.append(grand_parent_id)

            # æ›´æ–°ä¸‹ä¸€è½® ID
            next_batch_ids = current_generation_ids

        return expanded_docs

    def _expand_links(self, anchors: List[Document], query_vec: List[float], existing_pks: Set[str]) -> List[Document]:
        """
        æ‰©å±•æ–‡æ¡£å†…éƒ¨çš„å…³è”é“¾æ¥ (Related Links)
        é€»è¾‘ï¼šè·å–æ‰€æœ‰ Link -> è®¡ç®— Sim(Link, Query) -> Top-L æˆªæ–­
        """

        # 1. æ”¶é›†æ‰€æœ‰å¾…é€‰é“¾æ¥ ID
        # Map: link_pk -> (link_text, source_anchor_text)
        link_candidates = {}

        for doc in anchors:
            # related_links æ˜¯ list of dict: [{'pk':..., 'text':..., 'type':...}]
            links = doc.metadata.get("related_links", [])
            if not links:
                continue

            for link in links:
                target_pk = link.get("pk")
                l_type = link.get("type")
                l_text = link.get("text", "link")
                # åªå¤„ç†å†…éƒ¨é“¾æ¥ä¸”æœªè¢«æ”¶å½•çš„
                if target_pk and l_type == "internal" and target_pk not in existing_pks:
                    # å¦‚æœåŒä¸€ä¸ªæ–‡æ¡£è¢«å¤šæ¬¡å¼•ç”¨ï¼Œä¿ç•™ä»»æ„ä¸€ä¸ªæ¥æºå³å¯
                    link_candidates[target_pk] = (l_text, doc.page_content[:50])

        if not link_candidates:
            return []

        # 2. æ‰¹é‡æ‹‰å–é“¾æ¥æ–‡æ¡£
        candidate_pks = list(link_candidates.keys())
        fetched_docs = self._batch_fetch(candidate_pks)

        # 3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶è¯„åˆ†
        scored_candidates = []
        for doc in fetched_docs:
            pk = doc.metadata.get("pk")
            link_text, source_text = link_candidates.get(pk, ("unknown", "unknown"))

            summary_vec = doc.metadata.get("summary_vector")
            if not summary_vec:
                continue

            score = self._cosine_sim(query_vec, summary_vec)

            # è®°å½•å¿…è¦ä¿¡æ¯ä»¥ä¾¿æ’åº
            scored_candidates.append({
                "doc": doc,
                "score": score,
                "link_text": link_text,
                "source_text": source_text
            })

        # 4. åŠ¨æ€è®¡ç®— Top-L
        # max(1, min(5, ceil(len * proportion)))
        import math
        total_candidates = len(scored_candidates)
        top_l = max(1, min(self.max_link_top_k, math.ceil(total_candidates * self.link_proportion)))

        # 5. æ’åºæˆªæ–­
        scored_candidates.sort(key=lambda x: x["score"], reverse=True)
        selected_items = scored_candidates[:top_l]

        final_docs = []
        for item in selected_items:
            doc = item["doc"]
            # æ³¨å…¥ Metadata
            doc.metadata["expansion_type"] = "link"
            doc.metadata["expansion_source"] = f"Linked via '{item['link_text']}' from anchor '{item['source_text']}...'"
            doc.metadata["expansion_score"] = float(item["score"])

            existing_pks.add(doc.metadata.get("pk"))  # æ›´æ–°å»é‡é›†åˆ
            final_docs.append(doc)

        return final_docs

    def _batch_fetch(self, pks: List[str]) -> List[Document]:
        """
        ä» Milvus æ‰¹é‡è·å–æ–‡æ¡£
        """
        if not pks:
            return []

        try:
            col = Collection(self.collection_name, using=self.alias)
            # æ„é€ è¡¨è¾¾å¼
            expr = f"pk in {str(pks)}"

            # éœ€è¦æ‹‰å–çš„å­—æ®µ
            output_fields = [
                "pk", "text", "title", "parent_id", "summary_vector",
                "node_type", "related_links", "source"
            ]

            res = col.query(expr, output_fields=output_fields)

            # è½¬æ¢ä¸º Document å¯¹è±¡
            documents = []
            for hit in res:
                content = hit.get("text", "")
                # ç§»é™¤ vector å­—æ®µä»¥èŠ‚çœå†…å­˜ (é™¤éä¸‹ä¸€è½®éœ€è¦)
                # è¿™é‡Œæˆ‘ä»¬éœ€è¦ summary_vector è®¡ç®—ç›¸ä¼¼åº¦ï¼Œä¿ç•™åœ¨ metadata ä¸­
                meta = {k: v for k, v in hit.items() if k != "text"}
                doc = Document(page_content=content, metadata=meta)
                documents.append(doc)

            return documents

        except Exception as e:
            logger.error(f"Milvus batch fetch failed: {e}")
            return []

    @staticmethod
    def _cosine_sim(vec_a: List[float], vec_b: List[float]) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # è½¬æ¢ä¸º numpy æ•°ç»„
        a = np.array(vec_a)
        b = np.array(vec_b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)

        if norm_a == 0 or norm_b == 0:
            return 0.0

        return float(np.dot(a, b) / (norm_a * norm_b))