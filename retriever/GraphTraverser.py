import json
import logging
import math
from typing import List, Set, Dict, Optional

import numpy as np
from langchain_core.documents import Document
from pymilvus import Collection

from retriever import MilvusHybridRetriever
from utils import generate_node_id
from utils.milvus_adapter import HYBRID_SEARCH_FIELDS, decode_hit_to_document, decode_query_result_to_document


class GraphTraverser:
    """
    å›¾æ‹“æ‰‘æ‰©å±•å™¨
    è´Ÿè´£åŸºäºåˆå§‹é”šç‚¹æ–‡æ¡£ (Anchors) è¿›è¡Œçˆ¶çº§é€’å½’æ‰©å±•å’Œé“¾æ¥æ‹“æ‰‘æ‰©å±•
    """
    # é…ç½®æ—¥å¿—
    logger = logging.getLogger(__name__)
    def __init__(
            self,
            milvus_collection_name: str,
            milvus_connection_alias: str = "default",
            partition_names: Optional[List[str]] = None,
            parent_decay_threshold: float = 0.75,
            absolute_min_similarity: float = 0.2,  # é˜²æ­¢ç›¸å…³æ€§å¤ªä½
            link_proportion: float = 0.75,
            max_link_top_k: int = 10
    ):
        self.collection = Collection(milvus_collection_name, using=milvus_connection_alias)
        self.partition_names = partition_names
        # é˜ˆå€¼é…ç½®
        self.decay_threshold = parent_decay_threshold
        self.min_sim = absolute_min_similarity
        self.link_proportion = link_proportion
        self.max_link_top_k = max_link_top_k

    def expand(self, anchors: List[Document], query_vec: List[float]) -> List[Document]:
        """
        æ‰§è¡Œå›¾æ‰©å±•çš„ä¸»å…¥å£
        Args:
            anchors: åˆå§‹æ£€ç´¢åˆ°çš„é”šç‚¹æ–‡æ¡£åˆ—è¡¨
            query_vec: é¢„è®¡ç®—å¥½çš„ Query ç¨ å¯†å‘é‡ (æ¥è‡ª RetrievalNode çš„ç¼“å­˜)
        """
        if not anchors:
            return []

        # å»ºç«‹å·²å­˜åœ¨ ID é›†åˆï¼Œç”¨äºå»é‡
        existing_pks = {doc.metadata.get("pk") for doc in anchors if doc.metadata.get("pk")}

        # 1. çˆ¶çº§é€’å½’æ‰©å±• (åŸºäº Title é¢åŒ…å±‘ï¼Œä¼˜å…ˆæ‰§è¡Œä»¥ç¡®ç«‹ä¸Šä¸‹æ–‡ Scope)
        parent_docs = self._expand_parents(anchors, query_vec, existing_pks)
        print(f"   â¬†ï¸  Parent Expansion: Found {len(parent_docs)} docs")

        # 2. é“¾æ¥ä¸å…„å¼Ÿæ‰©å±• (åŸºäº Milvus Searchï¼Œè¡¥å……å…³è”ä¿¡æ¯)
        link_docs = self._expand_links(anchors, query_vec, existing_pks)
        print(f"   ğŸ”— Link Expansion: Found {len(link_docs)} docs")

        # 3. åˆå¹¶ç»“æœ (æ­¤æ—¶æ‰€æœ‰æ–‡æ¡£å·²å»é‡ä¸”æ ‡è®°äº† metadata)
        # é€šå¸¸ Traverser è¿”å›æ‰©å±•éƒ¨åˆ†ï¼Œç”±è°ƒç”¨æ–¹åˆå¹¶ã€‚ä¸ºäº†æ–¹ä¾¿ï¼Œè¿™é‡Œè¿”å› List[ExpandedDoc]
        return parent_docs + link_docs

    def _expand_parents(self, anchors: List[Document], query_vec: List[float], existing_pks: Set[str]) -> List[Document]:
        """
        åŸºäº Title é¢åŒ…å±‘ç»“æ„ä¸€æ¬¡æ€§æº¯æºï¼Œå‘ä¸Šæ‰©å±•çˆ¶èŠ‚ç‚¹
        é€»è¾‘ï¼šSim_j >= Sim_child * Threshold
        """
        # 1. è®¡ç®—æ‰€æœ‰æ½œåœ¨çš„ç¥–å…ˆèŠ‚ç‚¹ ID
        # Map: anchor_pk -> ancestor_ids (æŒ‰ parent -> root æ’åº)
        lineage_map: Dict[str, List[str]] = {}
        all_ancestor_pks = set()

        for doc in anchors:
            pk = doc.metadata.get("pk")
            title = doc.metadata.get("title")
            parts = title.split('_')    # åˆ†å‰²æ ‡é¢˜

            if not pk or not title or len(parts) <= 1:
                continue  # æ²¡æœ‰çˆ¶èŠ‚ç‚¹ï¼ˆå·²ç»æ˜¯æ ¹æˆ–æ— ç»“æ„ï¼‰

            ancestor_ids = []
            # ä»é•¿åˆ°çŸ­åˆ‡åˆ†ï¼Œç¡®ä¿é¡ºåºæ˜¯ï¼šç›´æ¥çˆ¶èŠ‚ç‚¹ -> ... -> æ ¹èŠ‚ç‚¹ã€‚indices: len-1, len-2, ... 1
            for i in range(len(parts) - 1, 0, -1):
                parent_title_str = "_".join(parts[:i])
                parent_id = generate_node_id(parent_title_str)
                ancestor_ids.append(parent_id)

            lineage_map[pk] = ancestor_ids
            all_ancestor_pks.update(ancestor_ids)

        if not all_ancestor_pks:
            return []

        # 2. æ‰¹é‡æ‹‰å–æ‰€æœ‰ç¥–å…ˆæ–‡æ¡£ (1æ¬¡ IO)
        # è¿‡æ»¤æ‰å·²ç»æ˜¯ Anchor è‡ªèº«çš„æ–‡æ¡£ (ç†è®ºä¸Š generate_node_id ä¸ä¼šå†²çªï¼Œä½†ä¸ºäº†å®‰å…¨)
        fetch_list = list(all_ancestor_pks - existing_pks)
        fetched_docs = self.batch_fetch(fetch_list)
        doc_lookup = {d.metadata.get("pk"): d for d in fetched_docs}    # å»ºç«‹å€’æŸ¥è¡¨: pk -> Document

        # 3. å†…å­˜ä¸­æ‰§è¡Œè¯­ä¹‰è¡°å‡æ£€æŸ¥
        expanded_docs = []
        for anchor in anchors:
            anchor_pk = anchor.metadata.get("pk")
            ancestors = lineage_map.get(anchor_pk, [])  # å·²æŒ‰ parent -> root æ’åº

            # è·å– Anchor è‡ªèº«ç›¸ä¼¼åº¦ä½œä¸ºåŸºå‡†
            anchor_summary = anchor.metadata.get("summary_vector")
            child_sim = self._cosine_sim(query_vec, anchor_summary) if anchor_summary else 0.5

            # å½“å‰è¿™ä¸€ä»£çš„â€œå­èŠ‚ç‚¹â€åˆ†æ•°ï¼Œåˆå§‹ä¸º Anchor çš„åˆ†æ•°
            current_child_score = child_sim
            current_child_text = anchor.metadata.get("title", anchor.page_content[:50])

            for ancestor_pk in ancestors:
                parent_doc = doc_lookup.get(ancestor_pk)
                # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨å¹¶ä¸”æ‘˜è¦å‘é‡æ˜¯å¦å­˜åœ¨ï¼ŒåŒæ—¶èµ‹å€¼ç»™ summary_vec
                if not parent_doc or not (summary_vec := parent_doc.metadata.get("summary_vector")):
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦ä¸åˆ¤æ–­é˜ˆå€¼: Parent å¿…é¡»è¾¾åˆ° Child * Threshold
                sim_j = self._cosine_sim(query_vec, summary_vec)
                required_score = current_child_score * self.decay_threshold

                if sim_j >= required_score and sim_j > self.min_sim:
                    if ancestor_pk not in existing_pks:     # è¾¾æ ‡ï¼šåŠ å…¥ç»“æœï¼Œå…ƒæ•°æ®å¢å¼º
                        parent_doc.metadata["source_type"] = "parent"
                        parent_doc.metadata["source_desc"] = f"Parent of: '{current_child_text}'"
                        # è®°å½•å¹¶æ‰©å±•
                        existing_pks.add(ancestor_pk)
                        expanded_docs.append(parent_doc)

                    # æ›´æ–°çŠ¶æ€ï¼Œå‡†å¤‡åˆ¤æ–­ä¸‹ä¸€çº§ (GrandParent)
                    current_child_score = sim_j
                    current_child_text = parent_doc.metadata.get("title", "parent_node")
                else:
                    break   # è¡°å‡é˜»æ–­ï¼šå¦‚æœè¿™ä¸€çº§çˆ¶èŠ‚ç‚¹ä¸ç›¸å…³ï¼Œä¸å†ç»§ç»­å‘ä¸Šè¿½æº¯æ ¹èŠ‚ç‚¹ï¼Œé¿å…æ‹‰å…¥æ— å…³ Root

        return expanded_docs

    def _expand_links(self, anchors: List[Document], query_vec: List[float], existing_pks: Set[str]) -> List[Document]:
        """
        æ‰©å±•æ–‡æ¡£å†…éƒ¨çš„å…³è”é“¾æ¥ (Related Links)å’Œå…„å¼ŸèŠ‚ç‚¹ (Siblings)
        é€»è¾‘ï¼šè·å–æ‰€æœ‰ Link -> è®¡ç®— Sim(Link, Query) -> Top-L æˆªæ–­
        """
        # Map: candidate_pk -> (source_anchor_text, relationship_type)ï¼Œç”¨äºåç»­ç»™å¬å›æ–‡æ¡£æ‰“æ ‡
        candidate_map = {}

        for doc in anchors:
            source_title = doc.metadata.get("title")

            # 1. å…ˆå¤„ç†å…„å¼ŸèŠ‚ç‚¹ (ä¼˜å…ˆçº§è¾ƒä½ï¼Œä½œä¸º Base)
            prev_id: str = doc.metadata.get("left_sibling")
            next_id: str = doc.metadata.get("right_sibling")

            if prev_id and prev_id not in existing_pks and prev_id not in candidate_map:
                candidate_map[prev_id] = (f"Previous of '{source_title}'", "sibling")
            if next_id and next_id not in existing_pks and next_id not in candidate_map:
                candidate_map[next_id] = (f"Next of '{source_title}'", "sibling")

            # 2. åå¤„ç†å¼•ç”¨é“¾æ¥ (ä¼˜å…ˆçº§è¾ƒé«˜ï¼Œè¦†å†™ or èåˆ)
            for link in doc.metadata.get("related_links", []):
                if isinstance(link, dict):
                    target_pk: str = link.get("pk")
                    l_type = link.get("type")
                    l_text = link.get("text", "link")

                    if target_pk and l_type == "internal" and target_pk not in existing_pks:
                        # æ„é€ å¼ºè¯­ä¹‰æè¿°
                        link_desc = f"Linked via '{l_text}' from '{source_title}'"

                        # é€»è¾‘ï¼šæ— è®ºä¹‹å‰æ˜¯å¦ä½œä¸ºå…„å¼ŸèŠ‚ç‚¹æ·»åŠ è¿‡ï¼Œè¿™é‡Œéƒ½è¿›è¡Œè¦†ç›–æˆ–å¢å¼º
                        # å› ä¸ºé”šç‚¹æ–‡æœ¬ (l_text) å¯¹ Rerank çš„ä»·å€¼è¿œå¤§äº "Next step"
                        if target_pk in candidate_map:
                            # ã€é«˜é˜¶ç­–ç•¥ã€‘å¦‚æœå·²ç»å­˜åœ¨ï¼ˆè¯´æ˜æ—¢æ˜¯å…„å¼Ÿåˆæ˜¯å¼•ç”¨ï¼‰ï¼Œå¯ä»¥åˆå¹¶æè¿°
                            old_desc, old_type = candidate_map[target_pk]
                            if "sibling" in old_type:
                                merged_desc = f"{link_desc} (also {old_desc})"
                                candidate_map[target_pk] = (merged_desc, "link")
                        else:
                            candidate_map[target_pk] = (link_desc, "link")  # æœªæ£€ç´¢ï¼Œç›´æ¥æ·»åŠ 

        if not candidate_map:
            return []

        candidate_pks: list[str] = list(candidate_map.keys())

        # 3. åŠ¨æ€è®¡ç®— Top-L
        # P: æ•°æ®é©±åŠ¨çš„ç›®æ ‡æ•°é‡ (å€™é€‰æ€»æ•°çš„ä¸€å®šæ¯”ä¾‹)
        # A. base_floor: åŸºç¡€ä¿åº•æ•°é‡ (1 + é”šç‚¹æ•°)ï¼Œä¿è¯æ¯ä¸ªé”šç‚¹è‡³å°‘æœ‰ä¸€ä¸ªæ‰©å±•æœºä¼š
        # K. max_link_top_k: ç³»ç»Ÿç¡¬æ€§ä¸Šé™ï¼Œé˜²æ­¢ Context çˆ†ç‚¸

        limit_by_prop = math.ceil(len(candidate_pks) * self.link_proportion)
        base_floor = 1 + len(anchors)
        top_l = min(self.max_link_top_k, max(base_floor, limit_by_prop))

        # 4. ä½¿ç”¨ Milvus è¿›è¡Œé«˜æ•ˆè¿‡æ»¤å’Œæ’åº
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ summary_vector è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼ˆé€šå¸¸æ›´è½»é‡ä¸”ä»£è¡¨æ€§å¼ºï¼‰
        # å¦‚æœ collection ä¸­ vector æ˜¯å…¨æ–‡å‘é‡ï¼Œsummary_vector æ˜¯æ‘˜è¦å‘é‡ï¼Œ
        # åœ¨åšâ€œé“¾æ¥æ¨èâ€æ—¶ï¼Œç”¨ Query åŒ¹é… Link çš„ Summary å¯èƒ½æ¯”åŒ¹é… Full Text æ›´å‡†ã€‚
        try:
            # æ„é€  expr: pk in ["a", "b", ...]ã€‚æ³¨æ„ Milvus expr å¯¹ list é•¿åº¦æœ‰é™åˆ¶ (é€šå¸¸ < 16384)ï¼Œè¿™é‡Œé€šå¸¸ä¸ä¼šè¶…
            expr = f"pk in {json.dumps(candidate_pks)}"
            search_params = MilvusHybridRetriever.dense_search_params

            # æ‰§è¡Œ Search
            res = self.collection.search(
                data=[query_vec],
                anns_field="summary_vector",
                param=search_params,
                limit=top_l,
                expr=expr,
                partition_names=self.partition_names,
                output_fields=HYBRID_SEARCH_FIELDS,
            )

            final_docs = []
            # 5. è§£æç»“æœ
            for hits in res:
                for hit in hits:
                    decoded_doc = decode_hit_to_document(hit, content_field="text")
                    pk = decoded_doc.metadata.get("pk")
                    # æ¢å¤æ¥æºä¸Šä¸‹æ–‡ï¼Œæ³¨å…¥æ‰©å±•å…ƒæ•°æ®
                    source_desc, source_type = candidate_map.get(pk, ("Unknown link", "link"))
                    decoded_doc.metadata["source_type"] = source_type
                    decoded_doc.metadata["source_desc"] = source_desc
                    # è®°å½•å¹¶æ‰©å±•
                    existing_pks.add(pk)
                    final_docs.append(decoded_doc)

            return final_docs

        except Exception as e:
            self.logger.error(f"Milvus link expansion search failed: {e}")
            # Fallback (å¯é€‰): å¦‚æœ search å¤±è´¥ï¼Œå¯ä»¥é™çº§å› batch fetchï¼Œä½†é€šå¸¸ search å¤±è´¥ fetch ä¹Ÿä¼šå¤±è´¥
            return []

    def batch_fetch(self, pks: List[str]) -> List[Document]:
        """
        ä» Milvus æ‰¹é‡è·å–æ–‡æ¡£
        """
        if not pks:
            return []

        try:
            expr = f"pk in {json.dumps(pks)}"   # æ„é€ è¡¨è¾¾å¼
            res = self.collection.query(expr, output_fields=HYBRID_SEARCH_FIELDS, partition_names=self.partition_names)
            return [decode_query_result_to_document(row, content_field="text") for row in res]

        except Exception as e:
            self.logger.error(f"Milvus batch fetch failed: {e}")
            return []

    @staticmethod
    def _cosine_sim(vec_a: List[float], vec_b: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        a = np.array(vec_a)
        b = np.array(vec_b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)

        if norm_a == 0 or norm_b == 0:
            return 0.0
        return float(np.dot(a, b) / (norm_a * norm_b))