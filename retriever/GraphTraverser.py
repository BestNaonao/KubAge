import logging
from typing import List, Set, Dict

import numpy as np
from langchain_core.documents import Document
from pymilvus import Collection

from utils import generate_node_id


class GraphTraverser:
    """
    å›¾æ‹“æ‰‘æ‰©å±•å™¨
    è´Ÿè´£åŸºäºåˆå§‹é”šç‚¹æ–‡æ¡£ (Anchors) è¿›è¡Œçˆ¶çº§é€’å½’æ‰©å±•å’Œé“¾æ¥æ‹“æ‰‘æ‰©å±•
    """
    # é…ç½®æ—¥å¿—
    logger = logging.getLogger(__name__)
    def __init__(
            self,
            milvus_collection_name: str,
            milvus_connection_alias: str = "default",
            parent_decay_threshold: float = 0.75,
            absolute_min_similarity: float = 0.2,  # é˜²æ­¢ç›¸å…³æ€§å¤ªä½
            link_proportion: float = 0.75,
            max_link_top_k: int = 5
    ):
        self.collection_name = milvus_collection_name
        self.alias = milvus_connection_alias

        # é˜ˆå€¼é…ç½®
        self.decay_threshold = parent_decay_threshold
        self.min_sim = absolute_min_similarity
        self.link_proportion = link_proportion
        self.max_link_top_k = max_link_top_k

    def expand(self, anchors: List[Document], query_vec: List[float]) -> List[Document]:
        """
        æ‰§è¡Œå›¾æ‰©å±•çš„ä¸»å…¥å£
        Args:
            anchors: åˆå§‹æ£€ç´¢åˆ°çš„é”šç‚¹æ–‡æ¡£åˆ—è¡¨
            query_vec: é¢„è®¡ç®—å¥½çš„ Query ç¨ å¯†å‘é‡ (æ¥è‡ª RetrievalNode çš„ç¼“å­˜)
        """
        if not anchors:
            return []

        # å»ºç«‹å·²å­˜åœ¨ ID é›†åˆï¼Œç”¨äºå»é‡
        existing_pks = {doc.metadata.get("pk") for doc in anchors if doc.metadata.get("pk")}

        # 1. çˆ¶çº§é€’å½’æ‰©å±•
        parent_docs = self._expand_parents(anchors, query_vec, existing_pks)
        print(f"   â¬†ï¸  Parent Expansion: Found {len(parent_docs)} docs")

        # 2. é“¾æ¥æ‹“æ‰‘æ‰©å±•
        link_docs = self._expand_links(anchors, query_vec, existing_pks)
        print(f"   ğŸ”— Link Expansion: Found {len(link_docs)} docs")

        # 3. åˆå¹¶ç»“æœ (æ­¤æ—¶æ‰€æœ‰æ–‡æ¡£å·²å»é‡ä¸”æ ‡è®°äº† metadata)
        # æ³¨æ„ï¼šè¿™é‡Œåªè¿”å›æ–°å¢çš„æ‰©å±•æ–‡æ¡£ï¼Œè¿˜æ˜¯è¿”å›å…¨éƒ¨ï¼Ÿ
        # é€šå¸¸ Traverser è¿”å›æ‰©å±•éƒ¨åˆ†ï¼Œç”±è°ƒç”¨æ–¹åˆå¹¶ã€‚ä½†ä¸ºäº†æ–¹ä¾¿ï¼Œè¿™é‡Œè¿”å› List[ExpandedDoc]
        return parent_docs + link_docs

    def _expand_parents(self, anchors: List[Document], query_vec: List[float], existing_pks: Set[str]) -> List[Document]:
        """
        åŸºäº Title é¢åŒ…å±‘ç»“æ„ä¸€æ¬¡æ€§æº¯æºï¼Œå‘ä¸Šæ‰©å±•çˆ¶èŠ‚ç‚¹
        é€»è¾‘ï¼šSim_j >= Sim_child * Threshold
        """
        # 1. è®¡ç®—æ‰€æœ‰æ½œåœ¨çš„ç¥–å…ˆèŠ‚ç‚¹ ID
        # Map: anchor_pk -> ancestor_ids (æŒ‰ parent -> root æ’åº)
        lineage_map: Dict[str, List[str]] = {}
        all_ancestor_pks = set()

        for doc in anchors:
            pk = doc.metadata.get("pk")
            title = doc.metadata.get("title")
            parts = title.split('_')    # åˆ†å‰²æ ‡é¢˜

            if not pk or not title or len(parts) <= 1:
                continue  # æ²¡æœ‰çˆ¶èŠ‚ç‚¹ï¼ˆå·²ç»æ˜¯æ ¹æˆ–æ— ç»“æ„ï¼‰

            ancestor_ids = []
            # ä»é•¿åˆ°çŸ­åˆ‡åˆ†ï¼Œç¡®ä¿é¡ºåºæ˜¯ï¼šç›´æ¥çˆ¶èŠ‚ç‚¹ -> ... -> æ ¹èŠ‚ç‚¹ã€‚indices: len-1, len-2, ... 1
            for i in range(len(parts) - 1, 0, -1):
                parent_title_str = "_".join(parts[:i])
                parent_id = generate_node_id(parent_title_str)
                ancestor_ids.append(parent_id)

            lineage_map[pk] = ancestor_ids
            all_ancestor_pks.update(ancestor_ids)

        if not all_ancestor_pks:
            return []

        # 2. æ‰¹é‡æ‹‰å–æ‰€æœ‰ç¥–å…ˆæ–‡æ¡£ (1æ¬¡ IO)
        # è¿‡æ»¤æ‰å·²ç»æ˜¯ Anchor è‡ªèº«çš„æ–‡æ¡£ (ç†è®ºä¸Š generate_node_id ä¸ä¼šå†²çªï¼Œä½†ä¸ºäº†å®‰å…¨)
        fetch_list = list(all_ancestor_pks - existing_pks)
        fetched_docs = self._batch_fetch(fetch_list)

        # å»ºç«‹é€ŸæŸ¥è¡¨: pk -> Document
        doc_lookup = {d.metadata.get("pk"): d for d in fetched_docs}

        # 3. å†…å­˜ä¸­æ‰§è¡Œè¯­ä¹‰è¡°å‡æ£€æŸ¥
        expanded_docs = []
        for anchor in anchors:
            anchor_pk = anchor.metadata.get("pk")
            ancestors = lineage_map.get(anchor_pk, [])  # å·²æŒ‰ parent -> root æ’åº

            # è·å– Anchor è‡ªèº«ç›¸ä¼¼åº¦ä½œä¸ºåŸºå‡†
            summary_vec = anchor.metadata.get("summary_vector")
            child_sim = self._cosine_sim(query_vec, summary_vec) if summary_vec else 0.5

            # å½“å‰è¿™ä¸€ä»£çš„â€œå­èŠ‚ç‚¹â€åˆ†æ•°ï¼Œåˆå§‹ä¸º Anchor çš„åˆ†æ•°
            current_child_score = child_sim
            current_child_text = anchor.metadata.get("title", anchor.page_content[:50])

            for ancestor_pk in ancestors:
                parent_doc = doc_lookup.get(ancestor_pk)

                # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨å¹¶ä¸”æ‘˜è¦å‘é‡æ˜¯å¦å­˜åœ¨ï¼ŒåŒæ—¶èµ‹å€¼ç»™ summary_vec
                if not parent_doc or not (summary_vec := parent_doc.metadata.get("summary_vector")):
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦ä¸åˆ¤æ–­é˜ˆå€¼: Parent å¿…é¡»è¾¾åˆ° Child * Threshold
                sim_j = self._cosine_sim(query_vec, summary_vec)
                required_score = current_child_score * self.decay_threshold

                if sim_j >= required_score and sim_j > self.min_sim:
                    if ancestor_pk not in existing_pks:     # è¾¾æ ‡ï¼šåŠ å…¥ç»“æœ
                        # æ³¨å…¥ Metadata
                        parent_doc.metadata["expansion_type"] = "parent"
                        parent_doc.metadata["expansion_source"] = f"Parent of: '{current_child_text}'"
                        parent_doc.metadata["expansion_score"] = float(sim_j)

                        existing_pks.add(ancestor_pk)
                        expanded_docs.append(parent_doc)

                    # æ›´æ–°çŠ¶æ€ï¼Œå‡†å¤‡åˆ¤æ–­ä¸‹ä¸€çº§ (GrandParent)
                    current_child_score = sim_j
                    current_child_text = parent_doc.metadata.get("title", "parent_node")
                else:
                    # è¡°å‡é˜»æ–­ï¼šå¦‚æœè¿™ä¸€çº§çˆ¶èŠ‚ç‚¹ä¸ç›¸å…³ï¼Œä¸å†ç»§ç»­å‘ä¸Šè¿½æº¯æ ¹èŠ‚ç‚¹ï¼Œé¿å…æŠŠæ— å…³çš„å…¨å±€ Root æ‹‰è¿›æ¥
                    break

        return expanded_docs

    def _expand_links(self, anchors: List[Document], query_vec: List[float], existing_pks: Set[str]) -> List[Document]:
        """
        æ‰©å±•æ–‡æ¡£å†…éƒ¨çš„å…³è”é“¾æ¥ (Related Links)
        é€»è¾‘ï¼šè·å–æ‰€æœ‰ Link -> è®¡ç®— Sim(Link, Query) -> Top-L æˆªæ–­
        """

        # 1. æ”¶é›†æ‰€æœ‰å¾…é€‰é“¾æ¥ ID
        # Map: link_pk -> (link_text, source_anchor_text)
        link_candidates = {}

        for doc in anchors:
            # related_links æ˜¯ list of dict: [{'pk':..., 'text':..., 'type':...}]
            links = doc.metadata.get("related_links", [])
            if not links:
                continue

            for link in links:
                target_pk = link.get("pk")
                l_type = link.get("type")
                l_text = link.get("text", "link")
                # åªå¤„ç†å†…éƒ¨é“¾æ¥ä¸”æœªè¢«æ”¶å½•çš„
                if target_pk and l_type == "internal" and target_pk not in existing_pks:
                    # å¦‚æœåŒä¸€ä¸ªæ–‡æ¡£è¢«å¤šæ¬¡å¼•ç”¨ï¼Œä¿ç•™ä»»æ„ä¸€ä¸ªæ¥æºå³å¯
                    link_candidates[target_pk] = (l_text, doc.page_content[:50])

        if not link_candidates:
            return []

        # 2. æ‰¹é‡æ‹‰å–é“¾æ¥æ–‡æ¡£
        candidate_pks = list(link_candidates.keys())
        fetched_docs = self._batch_fetch(candidate_pks)

        # 3. è®¡ç®—ç›¸ä¼¼åº¦å¹¶è¯„åˆ†
        scored_candidates = []
        for doc in fetched_docs:
            pk = doc.metadata.get("pk")
            link_text, source_text = link_candidates.get(pk, ("unknown", "unknown"))

            summary_vec = doc.metadata.get("summary_vector")
            if not summary_vec:
                continue

            score = self._cosine_sim(query_vec, summary_vec)

            # è®°å½•å¿…è¦ä¿¡æ¯ä»¥ä¾¿æ’åº
            scored_candidates.append({
                "doc": doc,
                "score": score,
                "link_text": link_text,
                "source_text": source_text
            })

        # 4. åŠ¨æ€è®¡ç®— Top-L
        # max(1, min(5, ceil(len * proportion)))
        import math
        total_candidates = len(scored_candidates)
        top_l = max(1, min(self.max_link_top_k, math.ceil(total_candidates * self.link_proportion)))

        # 5. æ’åºæˆªæ–­
        scored_candidates.sort(key=lambda x: x["score"], reverse=True)
        selected_items = scored_candidates[:top_l]

        final_docs = []
        for item in selected_items:
            doc = item["doc"]
            # æ³¨å…¥ Metadata
            doc.metadata["expansion_type"] = "link"
            doc.metadata["expansion_source"] = f"Linked via '{item['link_text']}' from anchor '{item['source_text']}...'"
            doc.metadata["expansion_score"] = float(item["score"])

            existing_pks.add(doc.metadata.get("pk"))  # æ›´æ–°å»é‡é›†åˆ
            final_docs.append(doc)

        return final_docs

    def _batch_fetch(self, pks: List[str]) -> List[Document]:
        """
        ä» Milvus æ‰¹é‡è·å–æ–‡æ¡£
        """
        if not pks:
            return []

        try:
            col = Collection(self.collection_name, using=self.alias)
            # æ„é€ è¡¨è¾¾å¼
            expr = f"pk in {str(pks)}"

            # éœ€è¦æ‹‰å–çš„å­—æ®µ
            output_fields = [
                "pk", "text", "title", "parent_id", "summary_vector",
                "node_type", "related_links", "source"
            ]

            res = col.query(expr, output_fields=output_fields)

            # è½¬æ¢ä¸º Document å¯¹è±¡
            documents = []
            for hit in res:
                content = hit.get("text", "")
                # ç§»é™¤ vector å­—æ®µä»¥èŠ‚çœå†…å­˜ (é™¤éä¸‹ä¸€è½®éœ€è¦)
                # è¿™é‡Œæˆ‘ä»¬éœ€è¦ summary_vector è®¡ç®—ç›¸ä¼¼åº¦ï¼Œä¿ç•™åœ¨ metadata ä¸­
                meta = {k: v for k, v in hit.items() if k != "text"}
                doc = Document(page_content=content, metadata=meta)
                documents.append(doc)

            return documents

        except Exception as e:
            self.logger.error(f"Milvus batch fetch failed: {e}")
            return []

    @staticmethod
    def _cosine_sim(vec_a: List[float], vec_b: List[float]) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        """
        # è½¬æ¢ä¸º numpy æ•°ç»„
        a = np.array(vec_a)
        b = np.array(vec_b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)

        if norm_a == 0 or norm_b == 0:
            return 0.0

        return float(np.dot(a, b) / (norm_a * norm_b))