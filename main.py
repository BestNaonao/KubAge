import asyncio
import os

import torch
from dotenv import load_dotenv, find_dotenv
from langchain_core.messages import HumanMessage
from langchain_huggingface import HuggingFaceEmbeddings
from pymilvus import connections
from pymilvus.model.hybrid import BGEM3EmbeddingFunction

from agent.graph import build_react_agent
from agent.nodes import RerankNode
from retriever import MilvusHybridRetriever
from utils.llm_factory import get_chat_model
from utils.mcp_manager import MCPToolManager


async def main():
    print("ğŸš€ Starting Kubernetes Agent...")
    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(find_dotenv())
    host = os.getenv('MILVUS_HOST', 'localhost')
    port = os.getenv('MILVUS_PORT', '19530')
    user = os.getenv('MILVUS_USER', 'root')
    password = os.getenv('MILVUS_ROOT_PASSWORD', 'Milvus')

    # --- A. è¿æ¥ Milvus
    print(f"æ­£åœ¨è¿æ¥ Milvus ({host}:{port})...")
    connections.connect(alias="default", host=host, port=port, user=user, password=password)

    # --- B. åˆå§‹åŒ–èµ„æº (ä¸€æ¬¡æ€§åŠ è½½æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½) ---
    print("â³ Initializing Embeddings and Retriever (this may take a while)...")

    # è¯·æ ¹æ®ä½ çš„å®é™…æ¨¡å‹è·¯å¾„ä¿®æ”¹
    DENSE_MODEL_PATH = "models/Qwen/Qwen3-Embedding-0.6B"
    SPARSE_MODEL_PATH = "BAAI/bge-m3"
    RERANKER_MODEL_PATH = "models/Qwen/Qwen3-Reranker-0.6B"
    COLLECTION_NAME = "knowledge_base_v2"

    # 1. Dense Embedding
    dense_embedding = HuggingFaceEmbeddings(
        model_name=DENSE_MODEL_PATH,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )

    # 2. Sparse Embedding
    sparse_embedding = BGEM3EmbeddingFunction(
        model_name=SPARSE_MODEL_PATH,
        use_fp16=True,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    # 3. Retriever
    retriever = MilvusHybridRetriever(
        collection_name=COLLECTION_NAME,
        dense_embedding_func=dense_embedding,
        sparse_embedding_func=sparse_embedding,
        top_k=5
    )

    reranker = RerankNode(RERANKER_MODEL_PATH, 5)

    # 1. åˆå§‹åŒ– MCP Manager
    mcp_manager = MCPToolManager.get_instance()
    # ç¡®ä¿ config/mcp_config.json å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®
    await mcp_manager.initialize(config_path="config/mcp_config.json")

    count = 0
    for tool_name, tool in mcp_manager.tools_map.items():
        print(f" -Tool: {tool_name}")
        count += 1
    if count < 26:
        print("   âŒ åŠ è½½MCPå·¥å…·å¤±è´¥:ç¼ºå°‘éƒ¨åˆ†å·¥å…·ï¼")
        await mcp_manager.close()
        return

    try:
        # è·å–å·¥å…·æè¿°æ–‡æœ¬
        tool_str = mcp_manager.get_tools_description()

        # 2. åˆå§‹åŒ–å…¶ä»–ç»„ä»¶ (LLM, Retriever ç­‰)
        llm = get_chat_model(
            temperature=0.1,
            extra_body={
                "top_k": 50,
                "thinking_budget": 32768,
            }
        )

        # 3. æ„å»º Agent
        app = build_react_agent(llm, retriever, reranker, tool_descriptions=tool_str)

        print("\nğŸš€ Agent Initialized. Ready for queries.")

        # 4. è¿è¡Œ Agent (ç¤ºä¾‹)
        inputs = {"messages": [HumanMessage(content="è¯·å¸®æˆ‘åœ¨workspace/binæ–‡ä»¶å¤¹ä¸‹ç”¨curlå®‰è£…kubectlã€‚")]}

        # ä½¿ç”¨ ainvoke å› ä¸º ToolNode æ˜¯å¼‚æ­¥çš„
        async for event in app.astream(inputs):
            for key, value in event.items():
                print(f"Completed Node: {key}")

    finally:
        # 5. æ¸…ç†èµ„æº
        await mcp_manager.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass