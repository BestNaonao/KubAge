from typing import Dict, Any

import torch
from langchain_core.runnables import RunnableConfig
from transformers import AutoModelForSequenceClassification, AutoTokenizer

from agent.state import AgentState


class RerankNode:
    def __init__(self, model_path: str, top_n: int = 5):
        """
        åˆå§‹åŒ–é‡æ’èŠ‚ç‚¹
        :param model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œå¦‚ ".model/Qwen/Qwen3-Reranker-0.6B"
        :param device: è¿è¡Œè®¾å¤‡ "cuda" or "cpu"
        :param top_n: é‡æ’åä¿ç•™çš„æ–‡æ¡£æ•°é‡
        """
        print(f"â³ Loading Reranker model from {model_path}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to("cuda" if torch.cuda.is_available() else "cpu")
        self.model.eval()

        self.top_n = top_n
        print("âœ… Reranker model loaded.")

    def __call__(self, state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
        print("\n--- [Rerank Node] Running ---")

        retrieved_docs = state.get("retrieved_chunks", [])
        if not retrieved_docs:
            print("âš ï¸ No documents to rerank.")
            return {"retrieved_chunks": []}

        # 1. ç¡®å®šé‡æ’ä½¿ç”¨çš„ Query
        # ç­–ç•¥ï¼šä½¿ç”¨ Analysis é˜¶æ®µç”Ÿæˆçš„ Technical Summary (æŠ€æœ¯æ‘˜è¦) ä½œä¸ºæœ€å‡†ç¡®çš„æŸ¥è¯¢æ„å›¾
        # å¦‚æœæ²¡æœ‰æ‘˜è¦ï¼Œå›é€€åˆ°ç”¨æˆ·åŸå§‹è¾“å…¥
        analysis = state.get("analysis")
        if analysis and analysis.technical_summary:
            query = analysis.technical_summary
            print(f"ğŸ¯ Using Technical Summary for reranking: {query[:50]}...")
        else:
            query = state["messages"][-1].content
            print(f"ğŸ¯ Using User Input for reranking: {query[:50]}...")

        # 2. æ„é€ æ¨¡å‹è¾“å…¥ pairs: [[query, doc1], [query, doc2], ...]
        pairs = [[query, doc.page_content] for doc in retrieved_docs]

        # 3. æ‰§è¡Œæ¨ç†æ‰“åˆ†
        with torch.no_grad():
            inputs = self.tokenizer(
                pairs,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=512
            ).to("cuda" if torch.cuda.is_available() else "cpu")

            scores = self.model(**inputs, return_dict=True).logits.view(-1, ).float()

        # 4. æ’åºä¸æˆªæ–­
        # å°†åˆ†æ•°ä¸æ–‡æ¡£ç»‘å®š
        doc_score_pairs = list(zip(retrieved_docs, scores.cpu().numpy()))

        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

        # ç­›é€‰ Top N
        reranked_docs = []
        print(f"ğŸ“Š Reranking Results (Top {self.top_n}):")
        for doc, score in doc_score_pairs[:self.top_n]:
            # å°†é‡æ’åˆ†æ•°å†™å…¥ metadataï¼Œæ–¹ä¾¿åç»­ debug
            doc.metadata["rerank_score"] = float(score)
            reranked_docs.append(doc)
            print(f"   Score: {score:.4f} | Source: {doc.metadata.get('source', 'unknown')}")

        return {"retrieved_chunks": reranked_docs}