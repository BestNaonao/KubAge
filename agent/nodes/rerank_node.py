from typing import List, Dict, Any

import torch
import torch.nn.functional as F
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

from agent.schemas import OperationType
from utils.document_schema import SourceType

RERANK_SYSTEM_PROMPT = """You are performing binary relevance judgment for retrieval reranking.
Determine whether the given Kubernetes documentation fragment contains authoritative and actionable information that can directly help answer or resolve the technical question described in the Query.

The Document may include a "[Retrieval Context]" label indicating how it was retrieved:
- Anchor/Direct-Hit: Judge direct topical and semantic relevance to the query.
- Parent/Context: Judge whether it provides necessary definitions, scope, or prerequisites for the query topic.
- Link/Sibling: Judge whether it offers essential troubleshooting steps or related configuration details missed by the Direct Hit.

Answer "yes" only if:
- The document provides concrete concepts, command references, configuration details, API semantics, or troubleshooting guidance or necessary context.
- The document meaningfully reduces uncertainty in solving the Query.

Answer "no" if:
- The Document is irrelevant or too generic even considering its retrieval context.
- The document does not independently contribute practical value to resolving the Query.

You must answer with exactly one word: yes or no."""


class RerankNode:
    def __init__(self, model_path: str, top_n: int = 5, max_length: int = 16384):
        """
        åˆå§‹åŒ– Qwen3-Reranker (CausalLM æ¨¡å¼)
        """
        print(f"â³ Loading Gen-Reranker model from {model_path}...")
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.top_n = top_n
        self.max_length = max_length

        # 1. åŠ è½½ Tokenizer (æ³¨æ„ padding_side='left')
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')

        # 2. åŠ è½½æ¨¡å‹ (AutoModelForCausalLM)
        # å¦‚æœæ˜¾å­˜å…è®¸ï¼Œæ¨èå¼€å¯ flash_attention_2
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                dtype=torch.float16 if self.device == "cuda" else torch.float32,
                attn_implementation="flash_attention_2" # æ˜¾å­˜å……è¶³ä¸”æ”¯æŒæ—¶å¯è§£å¼€æ³¨é‡Š
            ).to(self.device).eval()
        except Exception as e:
            print(f"âš ï¸ Failed to load with float16/flash_attn, falling back to default: {e}")
            self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device).eval()

        # 3. é¢„è®¡ç®— Prompt ç»„ä»¶
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")

        prefix = f"<|im_start|>system\n{RERANK_SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n"
        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"

        self.prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(suffix, add_special_tokens=False)

        # é’ˆå¯¹ä¸åŒæ“ä½œç±»å‹å®šåˆ¶å…³æ³¨ç‚¹
        self.base_instruct = "Given a technical query about Kubernetes, retrieve relevant documentation passages that provide answers or context."
        self.op_prompt_map = {
            # è¯Šæ–­åœºæ™¯ï¼šå…³æ³¨é”™è¯¯åŸå› ã€æ’æŸ¥æ­¥éª¤ã€å‘½ä»¤è¾“å‡ºè§£é‡Šã€æ—¥å¿—åˆ†æ
            OperationType.DIAGNOSIS: (
                "Given a troubleshooting scenario, retrieve documentation that explains error causes, "
                "debugging steps, log interpretation, or known issues related to the query. "
                "Prioritize actionable debugging guides over theoretical concepts. "
                "General debugging methodologies are acceptable when they clearly apply to the "
                "same failure category, even if specific resource or entity names are not mentioned."
            ),

            # åˆ é™¤/å±é™©æ“ä½œï¼šå…³æ³¨å‰¯ä½œç”¨ã€çº§è”å½±å“ã€å®‰å…¨æ“ä½œå‘½ä»¤ã€æ¢å¤æ–¹æ³•
            OperationType.RESOURCE_DELETION: (
                "Given a request to delete or remove resources, retrieve documentation that describes "
                "the deletion command syntax, potential side effects, cascading deletion policies (e.g., ownerReferences), "
                "and how to safely execute the removal."
            ),

            # é…ç½®å˜æ›´ï¼šå…³æ³¨ YAML å­—æ®µå®šä¹‰ã€spec ç»“æ„ã€é…ç½®é¡¹å«ä¹‰ã€å–å€¼èŒƒå›´
            OperationType.CONFIGURE: (
                "Given a configuration task, retrieve documentation that details the YAML resource definition, "
                "specific field semantics (under .spec), environment variables, or annotation options required "
                "to implement the requested configuration."
            ),

            # æ‰©ç¼©å®¹ï¼šå…³æ³¨ HPAã€replicas å­—æ®µã€èµ„æºé™åˆ¶(Limit/Request)ã€æ‰©å±•å‘½ä»¤
            OperationType.SCALING: (
                "Given a scaling or resource adjustment request, retrieve documentation concerning "
                "replica settings, HorizontalPodAutoscaler (HPA) configurations, 'kubectl scale' commands, "
                "or resource requests and limits strategies."
            ),

            # çŸ¥è¯†é—®ç­”ï¼šå…³æ³¨æ¦‚å¿µå®šä¹‰ã€æ¶æ„åŸç†ã€ç»„ä»¶å¯¹æ¯” (e.g. Deployment vs StatefulSet)
            OperationType.KNOWLEDGE_QA: (
                "Given a conceptual question, retrieve documentation that provides clear definitions, "
                "architectural overviews, component comparisons, or design principles. "
                "Prioritize comprehensive explanations over specific command syntax."
            ),

            # èµ„æºæŸ¥è¯¢ï¼šå…³æ³¨ kubectl get/describe ç”¨æ³•ã€JSONPathã€å­—æ®µå«ä¹‰
            OperationType.RESOURCE_INQUIRY: (
                "Given a request to query or view resource status, retrieve documentation about "
                "'kubectl get', 'kubectl describe', output formatting, or the meaning of specific "
                "status fields and conditions."
            ),

            # èµ„æºåˆ›å»ºï¼šå…³æ³¨ create/apply å‘½ä»¤ã€æœ€å°å¯ç”¨ YAML ç¤ºä¾‹
            OperationType.RESOURCE_CREATION: (
                "Given a resource creation task, retrieve documentation providing 'kubectl create/apply' examples, "
                "boilerplate YAML templates, or prerequisites for deploying the specified resource type."
            )
        }

        print("âœ… Gen-Reranker model loaded.")

    def _format_input_pair(self, instruction: str, query: str, doc: Document) -> str:
        """
        æ„é€ æ¨¡å‹è¾“å…¥ï¼š<Instruct> + <Query> + <Document (Title + Content)>
        """
        # åˆ©ç”¨æ–‡æ¡£å…ƒæ•°æ®ä¸­çš„ Title å¢å¼ºä¸Šä¸‹æ–‡
        title = doc.metadata.get("title", "Untitled Section")
        content = doc.page_content

        # æ˜¾å¼æ³¨å…¥ retrieval_source ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰ metadataï¼Œç»™é»˜è®¤å€¼
        source_type = doc.metadata.get("source_type", SourceType.UNKNOWN)
        source_desc = doc.metadata.get("source_desc", "Retrieved via vector search")

        # æ„é€ ä¸Šä¸‹æ–‡æè¿°å­—ç¬¦ä¸²ã€‚ä¾‹å¦‚: "[Retrieval Context]: Type=parent. Info=Parent of: 'Debug Service'"
        context_str = f"[Retrieval Context]: Type={source_type.value}. Info={source_desc}"

        # æ‹¼æ¥å¢å¼ºåçš„ Document å†…å®¹ã€‚å°† Context æ”¾åœ¨ Content ä¹‹å‰ï¼Œç¡®ä¿æ¨¡å‹å…ˆçœ‹åˆ°æ–‡æ¡£çš„å®šä½
        enriched_doc = f"[Title]: {title}\n{context_str}\n[Content]: {content}"

        return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {enriched_doc}"

    def _compute_scores(self, pairs: List[str], token_budget: int = 16384) -> List[float]:
        """
        åŸºäº Token Budget çš„åŠ¨æ€åˆ†æ‰¹æ¨ç†
        :param pairs: è¾“å…¥çš„æ–‡æœ¬å¯¹åˆ—è¡¨
        :param token_budget: æ˜¾å­˜å…è®¸çš„æœ€å¤§ token æ€»æ•° (Batch Size * Max Seq Len)
        :return: æŒ‰ç…§è¾“å…¥é¡ºåºæ’åˆ—çš„åˆ†æ•°åˆ—è¡¨
        """
        # 1. é¢„å¤„ç†ï¼šTokenize å¹¶æ‰‹åŠ¨æ‹¼æ¥ Prefix/Suffix
        # ä¸ºäº†æ•ˆç‡ï¼Œè¿™é‡Œä½¿ç”¨ batch_encode è¿™é‡Œçš„ truncation åªæ˜¯ä¸ºäº†é˜²æ­¢å•æ¡è¶…é•¿
        inputs = self.tokenizer(
            pairs,
            padding=False,
            truncation='longest_first',
            return_attention_mask=False,
            max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)
        )
        raw_input_ids = inputs['input_ids']

        # æ„é€ å¸¦ç´¢å¼•çš„æ•°æ®ï¼š(original_index, full_input_ids)
        indexed_data = []
        for idx, token_ids in enumerate(raw_input_ids):
            full_ids = self.prefix_tokens + token_ids + self.suffix_tokens
            indexed_data.append((idx, full_ids))

        # 2. æ’åºï¼šæŒ‰ç…§ token æ•°é‡é™åºæ’åˆ—
        # é™åºçš„å¥½å¤„ï¼šä¼˜å…ˆå¤„ç†æœ€é•¿çš„ï¼Œå¦‚æœæœ€é•¿çš„å•æ¡éƒ½çˆ†æ˜¾å­˜ï¼Œèƒ½å°½æ—©å‘ç°ï¼›ä¸”é€šå¸¸é•¿çŸ­æ–‡æœ¬åˆ†ç»„æ›´ç´§å‡‘
        indexed_data.sort(key=lambda x: len(x[1]), reverse=True)

        # 3. åŠ¨æ€åˆ†æ‰¹ (Greedy Batching based on Token Budget)
        batches = []
        current_batch = []
        current_max_len = 0

        for original_idx, ids in indexed_data:
            seq_len = len(ids)

            # è¯•æ¢æ€§è®¡ç®—ï¼šå¦‚æœæŠŠå½“å‰æ ·æœ¬åŠ å…¥ batchï¼Œæ–°çš„ batch å ç”¨å¤šå°‘æ˜¾å­˜ï¼Ÿ
            # æ˜¾å­˜å ç”¨ âˆ (å½“å‰Batchæ•°é‡ + 1) * max(å½“å‰æœ€å¤§é•¿åº¦, æ–°æ ·æœ¬é•¿åº¦)
            # å› ä¸ºæ˜¯é™åºæ’åˆ—ï¼Œæ–°æ ·æœ¬é•¿åº¦ä¸€å®š <= current_max_len (é™¤éæ˜¯ Batch çš„ç¬¬ä¸€ä¸ª)
            next_max_len = max(current_max_len, seq_len)
            next_batch_size = len(current_batch) + 1

            # è®¡ç®— Token æ¶ˆè€— (çŸ©å½¢é¢ç§¯)
            estimated_token_count = next_batch_size * next_max_len

            if estimated_token_count <= token_budget:
                # æ”¾å…¥å½“å‰ Batch
                current_batch.append((original_idx, ids))
                current_max_len = next_max_len
            else:
                # è¶…å‡ºé¢„ç®—ï¼Œå°åŒ…å½“å‰ Batchï¼Œå¼€å¯ä¸‹ä¸€ä¸ªæ–° Batch åˆå§‹åŒ–
                if current_batch:
                    batches.append(current_batch)

                current_batch = [(original_idx, ids)]
                current_max_len = seq_len

        # å¤„ç†æœ€åä¸€ä¸ª Batch
        if current_batch:
            batches.append(current_batch)

        # 4. æ‰¹é‡æ¨ç†
        results = []  # å­˜å‚¨ (original_index, score)

        for batch in batches:
            # batch ç»“æ„: [(idx, ids), (idx, ids), ...]
            batch_indices = [item[0] for item in batch]
            batch_ids = [item[1] for item in batch]

            # Pad å½“å‰ Batch (æ­¤æ—¶ batch å†…é•¿åº¦å·®å¼‚å¾ˆå°ï¼Œpadding å¾ˆå°‘)
            padded_inputs = self.tokenizer.pad(
                {'input_ids': batch_ids},
                padding=True,
                return_tensors="pt"
            )

            # Move to device
            for key in padded_inputs:
                padded_inputs[key] = padded_inputs[key].to(self.device)

            # Inference
            with torch.no_grad():
                outputs = self.model(**padded_inputs)
                batch_logits = outputs.logits[:, -1, :]

                true_vec = batch_logits[:, self.token_true_id]
                false_vec = batch_logits[:, self.token_false_id]

                combined = torch.stack([false_vec, true_vec], dim=1)
                probs = F.log_softmax(combined, dim=1)
                batch_scores = probs[:, 1].exp().tolist()

            # æ”¶é›†ç»“æœ
            for idx, score in zip(batch_indices, batch_scores):
                results.append((idx, score))

        # 5. é¡ºåºè¿˜åŸ (Reorder)
        # æŒ‰ç…§ original_index ä»å°åˆ°å¤§æ’åºï¼Œæ¢å¤åŸå§‹é¡ºåºï¼Œåªæå–åˆ†æ•°
        results.sort(key=lambda x: x[0])
        final_scores = [item[1] for item in results]

        return final_scores

    def __call__(self, state: dict, config: RunnableConfig) -> Dict[str, Any]:
        print("\n--- [Gen-Rerank Node] Running ---")

        retrieved_docs = state.get("retrieved_docs", [])
        if len(retrieved_docs) <= 0:
            return {"retrieved_docs": []}

        print(f"Max Token Count: {max([doc.metadata['token_count'] for doc in retrieved_docs])}")

        # ç¡®å®š Query (ä¼˜å…ˆä½¿ç”¨ Analysis é˜¶æ®µçš„æŠ€æœ¯æ‘˜è¦)
        analysis = state.get("analysis")

        if analysis:
            # ä¼˜å…ˆä½¿ç”¨æŠ€æœ¯æ‘˜è¦
            query_text = analysis.technical_summary
            # è·å–æ“ä½œç±»å‹
            op_type = analysis.target_operation
            print(f"ğŸ¯ Context: {op_type} | Query: {query_text[:100]}...")
        else:
            query_text = state["messages"][-1].content
            op_type = None
            print(f"ğŸ¯ Context: Raw Input | Query: {query_text[:100]}...")

        # æ ¹æ®æ“ä½œç±»å‹ç”ŸæˆåŠ¨æ€æŒ‡ä»¤ï¼Œæé«˜é‡æ’é’ˆå¯¹æ€§
        dynamic_instruction = self.op_prompt_map.get(op_type, self.base_instruct)
        print(f"ğŸ“‹ Instruction: {dynamic_instruction}")

        # å‡†å¤‡æ•°æ®å¯¹
        input_texts = [
            self._format_input_pair(dynamic_instruction, query_text, doc)
            for doc in retrieved_docs
        ]

        try:
            # è®¡ç®—åˆ†æ•°
            scores = self._compute_scores(input_texts)

            # ç»‘å®šåˆ†æ•°å¹¶æ’åº
            doc_score_pairs = list(zip(retrieved_docs, scores))
            doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

            print(f"ğŸ“Š Reranking Results (Top {self.top_n}):")
            reranked_docs = []
            for doc, score in doc_score_pairs[:self.top_n]:
                doc.metadata["rerank_score"] = float(score)
                reranked_docs.append(doc)
                print(f"   [{doc.metadata.get('source_type', 'UNK')}] Score: {score:.4f} | Title: {doc.metadata.get('title')}")

            return {"retrieved_docs": reranked_docs}

        except Exception as e:
            print(f"âŒ Rerank Failed: {e}")
            # å¦‚æœé‡æ’å¤±è´¥ï¼Œé™çº§è¿”å›åŸå§‹ç»“æœçš„å‰ N ä¸ª
            return {"retrieved_docs": retrieved_docs[:self.top_n]}