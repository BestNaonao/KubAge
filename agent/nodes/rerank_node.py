from typing import List, Dict, Any

import torch
import torch.nn.functional as F
from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

from agent.prompts import RERANK_SYSTEM_PROMPT
from agent.schemas import OperationType
from agent.state import AgentState


class RerankNode:
    def __init__(self, model_path: str, top_n: int = 5, max_length: int = 8192):
        """
        åˆå§‹åŒ– Qwen3-Reranker (CausalLM æ¨¡å¼)
        """
        print(f"â³ Loading Gen-Reranker model from {model_path}...")
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.top_n = top_n
        self.max_length = max_length

        # 1. åŠ è½½ Tokenizer (æ³¨æ„ padding_side='left')
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')

        # 2. åŠ è½½æ¨¡å‹ (AutoModelForCausalLM)
        # å¦‚æœæ˜¾å­˜å…è®¸ï¼Œæ¨èå¼€å¯ flash_attention_2
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                dtype=torch.float16 if self.device == "cuda" else torch.float32,
                attn_implementation="flash_attention_2" # æ˜¾å­˜å……è¶³ä¸”æ”¯æŒæ—¶å¯è§£å¼€æ³¨é‡Š
            ).to(self.device).eval()
        except Exception as e:
            print(f"âš ï¸ Failed to load with float16/flash_attn, falling back to default: {e}")
            self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device).eval()

        # 3. é¢„è®¡ç®— Prompt ç»„ä»¶
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")

        prefix = f"<|im_start|>system\n{RERANK_SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n"
        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"

        self.prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(suffix, add_special_tokens=False)

        # é’ˆå¯¹ä¸åŒæ“ä½œç±»å‹å®šåˆ¶å…³æ³¨ç‚¹
        self.base_instruct = "Given a technical query about Kubernetes, retrieve relevant documentation passages that provide answers or context."

        self.op_prompt_map = {
            # è¯Šæ–­åœºæ™¯ï¼šå…³æ³¨é”™è¯¯åŸå› ã€æ’æŸ¥æ­¥éª¤ã€å‘½ä»¤è¾“å‡ºè§£é‡Šã€æ—¥å¿—åˆ†æ
            OperationType.DIAGNOSIS: (
                "Given a troubleshooting scenario, retrieve documentation that explains error causes, "
                "debugging steps, log interpretation, or known issues related to the query. "
                "Prioritize actionable debugging guides over theoretical concepts."
            ),

            # åˆ é™¤/å±é™©æ“ä½œï¼šå…³æ³¨å‰¯ä½œç”¨ã€çº§è”å½±å“ã€å®‰å…¨æ“ä½œå‘½ä»¤ã€æ¢å¤æ–¹æ³•
            OperationType.RESOURCE_DELETION: (
                "Given a request to delete or remove resources, retrieve documentation that describes "
                "the deletion command syntax, potential side effects, cascading deletion policies (e.g., ownerReferences), "
                "and how to safely execute the removal."
            ),

            # é…ç½®å˜æ›´ï¼šå…³æ³¨ YAML å­—æ®µå®šä¹‰ã€spec ç»“æ„ã€é…ç½®é¡¹å«ä¹‰ã€å–å€¼èŒƒå›´
            OperationType.CONFIGURE: (
                "Given a configuration task, retrieve documentation that details the YAML resource definition, "
                "specific field semantics (under .spec), environment variables, or annotation options required "
                "to implement the requested configuration."
            ),

            # æ‰©ç¼©å®¹ï¼šå…³æ³¨ HPAã€replicas å­—æ®µã€èµ„æºé™åˆ¶(Limit/Request)ã€æ‰©å±•å‘½ä»¤
            OperationType.SCALING: (
                "Given a scaling or resource adjustment request, retrieve documentation concerning "
                "replica settings, HorizontalPodAutoscaler (HPA) configurations, 'kubectl scale' commands, "
                "or resource requests and limits strategies."
            ),

            # çŸ¥è¯†é—®ç­”ï¼šå…³æ³¨æ¦‚å¿µå®šä¹‰ã€æ¶æ„åŸç†ã€ç»„ä»¶å¯¹æ¯” (e.g. Deployment vs StatefulSet)
            OperationType.KNOWLEDGE_QA: (
                "Given a conceptual question, retrieve documentation that provides clear definitions, "
                "architectural overviews, component comparisons, or design principles. "
                "Prioritize comprehensive explanations over specific command syntax."
            ),

            # èµ„æºæŸ¥è¯¢ï¼šå…³æ³¨ kubectl get/describe ç”¨æ³•ã€JSONPathã€å­—æ®µå«ä¹‰
            OperationType.RESOURCE_INQUIRY: (
                "Given a request to query or view resource status, retrieve documentation about "
                "'kubectl get', 'kubectl describe', output formatting, or the meaning of specific "
                "status fields and conditions."
            ),

            # èµ„æºåˆ›å»ºï¼šå…³æ³¨ create/apply å‘½ä»¤ã€æœ€å°å¯ç”¨ YAML ç¤ºä¾‹
            OperationType.RESOURCE_CREATION: (
                "Given a resource creation task, retrieve documentation providing 'kubectl create/apply' examples, "
                "boilerplate YAML templates, or prerequisites for deploying the specified resource type."
            )
        }

        print("âœ… Gen-Reranker model loaded.")

    def _format_input_pair(self, instruction: str, query: str, doc: Document) -> str:
        """
        æ„é€ æ¨¡å‹è¾“å…¥ï¼š<Instruct> + <Query> + <Document (Title + Content)>
        """
        # åˆ©ç”¨æ–‡æ¡£å…ƒæ•°æ®ä¸­çš„ Title å¢å¼ºä¸Šä¸‹æ–‡
        title = doc.metadata.get("title", "Untitled Section")
        content = doc.page_content

        # æ˜¾å¼æ‹¼æ¥æ ‡é¢˜ï¼Œè¿™å¯¹ Reranker æå…¶é‡è¦
        enriched_doc = f"Title: {title}\nContent: {content}"

        return f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {enriched_doc}"

    def _compute_scores(self, pairs: List[str]) -> List[float]:
        """
        æ ¸å¿ƒæ‰“åˆ†é€»è¾‘ï¼šæ‰‹åŠ¨æ‹¼æ¥ tokens å¹¶è®¡ç®— yes/no æ¦‚ç‡
        """
        # 1. Tokenize query+doc pairs (ä¸å¸¦ padding)
        inputs = self.tokenizer(
            pairs,
            padding=False,
            truncation='longest_first',
            return_attention_mask=False,
            max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)
        )

        # 2. æ‰‹åŠ¨æ‹¼æ¥ prefix + content + suffix
        input_ids_list = inputs['input_ids']
        for i, token_ids in enumerate(input_ids_list):
            input_ids_list[i] = self.prefix_tokens + token_ids + self.suffix_tokens

        # 3. Batch Pad
        # tokenizer.pad ä¼šè‡ªåŠ¨ç”Ÿæˆ attention_mask å¹¶å¤„ç† left padding
        batch_inputs = self.tokenizer.pad(
            {'input_ids': input_ids_list},
            padding=True,
            return_tensors="pt"
        )

        # 4. Move to device
        for key in batch_inputs:
            batch_inputs[key] = batch_inputs[key].to(self.device)

        # 5. Inference
        with torch.no_grad():
            outputs = self.model(**batch_inputs)
            # å–æœ€åä¸€ä¸ª token çš„ logits
            batch_scores = outputs.logits[:, -1, :]

            # æå– yes å’Œ no çš„ logits
            true_vector = batch_scores[:, self.token_true_id]
            false_vector = batch_scores[:, self.token_false_id]

            # å †å å¹¶è®¡ç®— softmax
            combined_logits = torch.stack([false_vector, true_vector], dim=1)
            probs = F.log_softmax(combined_logits, dim=1)

            # å– index 1 ("yes") çš„æ¦‚ç‡ä½œä¸ºæœ€ç»ˆå¾—åˆ†
            scores = probs[:, 1].exp().tolist()

        return scores

    def __call__(self, state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
        print("\n--- [Gen-Rerank Node] Running ---")

        retrieved_docs = state.get("retrieved_chunks", [])
        if len(retrieved_docs) <= 1:
            return {"retrieved_chunks": []}

        # ç¡®å®š Query (ä¼˜å…ˆä½¿ç”¨ Analysis é˜¶æ®µçš„æŠ€æœ¯æ‘˜è¦)
        analysis = state.get("analysis")

        if analysis:
            # ä¼˜å…ˆä½¿ç”¨æŠ€æœ¯æ‘˜è¦
            query_text = analysis.technical_summary
            # è·å–æ“ä½œç±»å‹
            op_type = analysis.target_operation
            print(f"ğŸ¯ Context: {op_type} | Query: {query_text[:50]}...")
        else:
            query_text = state["messages"][-1].content
            op_type = None
            print(f"ğŸ¯ Context: Raw Input | Query: {query_text[:50]}...")

        # æ ¹æ®æ“ä½œç±»å‹ç”ŸæˆåŠ¨æ€æŒ‡ä»¤ï¼Œæé«˜é‡æ’é’ˆå¯¹æ€§
        dynamic_instruction = self.op_prompt_map.get(op_type, self.base_instruct)
        print(f"ğŸ“‹ Instruction: {dynamic_instruction}")

        # å‡†å¤‡æ•°æ®å¯¹
        input_texts = [
            self._format_input_pair(dynamic_instruction, query_text, doc)
            for doc in retrieved_docs
        ]

        try:
            # è®¡ç®—åˆ†æ•°
            scores = self._compute_scores(input_texts)

            # ç»‘å®šåˆ†æ•°å¹¶æ’åº
            doc_score_pairs = list(zip(retrieved_docs, scores))
            doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

            print(f"ğŸ“Š Reranking Results (Top {self.top_n}):")
            reranked_docs = []
            for doc, score in doc_score_pairs[:self.top_n]:
                doc.metadata["rerank_score"] = float(score)
                reranked_docs.append(doc)
                print(f"   Score: {score:.4f} | Source: {doc.metadata.get('source', 'unknown')}")

            return {"retrieved_chunks": reranked_docs}

        except Exception as e:
            print(f"âŒ Rerank Failed: {e}")
            # å¦‚æœé‡æ’å¤±è´¥ï¼Œé™çº§è¿”å›åŸå§‹ç»“æœçš„å‰ N ä¸ª
            return {"retrieved_chunks": retrieved_docs[:self.top_n]}