import torch
import torch.nn.functional as F
from typing import List, Dict, Any
from langchain_core.runnables import RunnableConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

from agent.state import AgentState
from agent.prompts import RERANK_SYSTEM_PROMPT


class RerankNode:
    def __init__(self, model_path: str, top_n: int = 5, max_length: int = 8192):
        """
        åˆå§‹åŒ– Qwen3-Reranker (CausalLM æ¨¡å¼)
        """
        print(f"â³ Loading Gen-Reranker model from {model_path}...")
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.top_n = top_n
        self.max_length = max_length

        # 1. åŠ è½½ Tokenizer (æ³¨æ„ padding_side='left')
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side='left')

        # 2. åŠ è½½æ¨¡å‹ (AutoModelForCausalLM)
        # å¦‚æœæ˜¾å­˜å…è®¸ï¼Œæ¨èå¼€å¯ flash_attention_2
        try:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                dtype=torch.float16 if self.device == "cuda" else torch.float32,
                attn_implementation="flash_attention_2" # æ˜¾å­˜å……è¶³ä¸”æ”¯æŒæ—¶å¯è§£å¼€æ³¨é‡Š
            ).to(self.device).eval()
        except Exception as e:
            print(f"âš ï¸ Failed to load with float16/flash_attn, falling back to default: {e}")
            self.model = AutoModelForCausalLM.from_pretrained(model_path).to(self.device).eval()

        # 3. é¢„è®¡ç®— Prompt ç»„ä»¶
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")

        prefix = f"<|im_start|>system\n{RERANK_SYSTEM_PROMPT}<|im_end|>\n<|im_start|>user\n"
        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"

        self.prefix_tokens = self.tokenizer.encode(prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(suffix, add_special_tokens=False)

        self.default_instruction = "Evaluate the document based on the system criteria."
        print("âœ… Gen-Reranker model loaded.")

    def _format_instruction(self, query: str, doc_content: str):
        return "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(
            instruction=self.default_instruction, query=query, doc=doc_content
        )

    def _compute_scores(self, pairs: List[str]) -> List[float]:
        """
        æ ¸å¿ƒæ‰“åˆ†é€»è¾‘ï¼šæ‰‹åŠ¨æ‹¼æ¥ tokens å¹¶è®¡ç®— yes/no æ¦‚ç‡
        """
        # 1. Tokenize query+doc pairs (ä¸å¸¦ padding)
        inputs = self.tokenizer(
            pairs,
            padding=False,
            truncation='longest_first',
            return_attention_mask=False,
            max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)
        )

        # 2. æ‰‹åŠ¨æ‹¼æ¥ prefix + content + suffix
        input_ids_list = inputs['input_ids']
        for i, ele in enumerate(input_ids_list):
            input_ids_list[i] = self.prefix_tokens + ele + self.suffix_tokens

        # 3. Batch Pad
        # tokenizer.pad ä¼šè‡ªåŠ¨ç”Ÿæˆ attention_mask å¹¶å¤„ç† left padding
        batch_inputs = self.tokenizer.pad(
            {'input_ids': input_ids_list},
            padding=True,
            return_tensors="pt"
        )

        # 4. Move to device
        for key in batch_inputs:
            batch_inputs[key] = batch_inputs[key].to(self.device)

        # 5. Inference
        with torch.no_grad():
            outputs = self.model(**batch_inputs)
            # å–æœ€åä¸€ä¸ª token çš„ logits
            batch_scores = outputs.logits[:, -1, :]

            # æå– yes å’Œ no çš„ logits
            true_vector = batch_scores[:, self.token_true_id]
            false_vector = batch_scores[:, self.token_false_id]

            # å †å å¹¶è®¡ç®— softmax
            combined_logits = torch.stack([false_vector, true_vector], dim=1)
            probs = F.log_softmax(combined_logits, dim=1)

            # å– index 1 ("yes") çš„æ¦‚ç‡ä½œä¸ºæœ€ç»ˆå¾—åˆ†
            scores = probs[:, 1].exp().tolist()

        return scores

    def __call__(self, state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
        print("\n--- [Gen-Rerank Node] Running ---")

        retrieved_docs = state.get("retrieved_chunks", [])
        if len(retrieved_docs) <= 1:
            return {"retrieved_chunks": []}

        # ç¡®å®š Query (ä¼˜å…ˆä½¿ç”¨ Analysis é˜¶æ®µçš„æŠ€æœ¯æ‘˜è¦)
        analysis = state.get("analysis")
        if analysis and analysis.technical_summary:
            query = analysis.technical_summary
            print(f"ğŸ¯ Query: {query[:50]}...")
        else:
            query = state["messages"][-1].content
            print(f"ğŸ¯ Query (Raw): {query[:50]}...")

        # å‡†å¤‡æ•°æ®å¯¹
        pairs = [self._format_instruction(query, doc.page_content) for doc in retrieved_docs]

        try:
            # è®¡ç®—åˆ†æ•°
            scores = self._compute_scores(pairs)

            # ç»‘å®šåˆ†æ•°å¹¶æ’åº
            doc_score_pairs = list(zip(retrieved_docs, scores))
            doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

            print(f"ğŸ“Š Reranking Results (Top {self.top_n}):")
            reranked_docs = []
            for doc, score in doc_score_pairs[:self.top_n]:
                doc.metadata["rerank_score"] = float(score)
                reranked_docs.append(doc)
                print(f"   Score: {score:.4f} | Source: {doc.metadata.get('source', 'unknown')}")

            return {"retrieved_chunks": reranked_docs}

        except Exception as e:
            print(f"âŒ Rerank Failed: {e}")
            # å¦‚æœé‡æ’å¤±è´¥ï¼Œé™çº§è¿”å›åŸå§‹ç»“æœçš„å‰ N ä¸ª
            return {"retrieved_chunks": retrieved_docs[:self.top_n]}