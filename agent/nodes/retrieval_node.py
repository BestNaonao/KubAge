import logging
from typing import List, Dict, Any, TypedDict, Optional

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig

from agent.nodes import RerankNode
from agent.schemas import ExecutionPlan, OperationType
from agent.state import AgentState
from retriever import MilvusHybridRetriever, GraphTraverser
from utils import csr_to_milvus_format
from utils.document_schema import SourceType
from workflow.build_knowledge_base import STATIC_PARTITION_NAME, DYNAMIC_PARTITION_NAME


class VectorSchema(TypedDict):
    dense: Optional[List[float]]
    sparse: Optional[Dict[int, float]]

class RetrievalNode:
    """
    æ£€ç´¢èŠ‚ç‚¹ï¼Œæ‰¹å¤„ç†å‘é‡åµŒå…¥ï¼Œå®ç°åŒè½¨åˆ¶ã€ä¸‰é˜¶æ®µæ£€ç´¢ï¼šé™æ€è½¨ä¸åŠ¨æ€è½¨ï¼Œç²—ç­›ï¼ˆRetrievalï¼‰ã€æ‰©å±•ï¼ˆExpansionï¼‰å’Œç²¾ç­›ï¼ˆRerankï¼‰
    """
    logger = logging.getLogger(__name__)
    priority_map = {
        SourceType.DYNAMIC: 4,
        SourceType.ANCHOR: 3,
        SourceType.PARENT: 2,
        SourceType.LINK: 1,
        SourceType.SIBLING: 1,
        SourceType.UNKNOWN: 0
    }       # æ–‡æ¡£æ¥æºçš„ä¼˜å…ˆçº§
    dynamic_track_ops = [OperationType.DIAGNOSIS, OperationType.RESOURCE_INQUIRY, OperationType.RESTART]

    def __init__(self, retriever: MilvusHybridRetriever, traverser: GraphTraverser, reranker: RerankNode):
        """
        åˆå§‹åŒ–æ£€ç´¢èŠ‚ç‚¹
        :param retriever: å·²åˆå§‹åŒ–çš„ MilvusHybridRetriever (æŒæœ‰ embedding models)
        :param traverser: å·²åˆå§‹åŒ–çš„ GraphTraverser (åªè´Ÿè´£æ‹“æ‰‘è®¡ç®—)
        :param reranker: å·²åˆå§‹åŒ–çš„ RerankNode
        """
        self.retriever = retriever
        self.traverser = traverser
        self.reranker = reranker

    def _batch_embed_queries(self, queries: List[str]) -> Dict[str, VectorSchema]:
        """
        Batch embed all queries using models from the retriever.
        """
        if not queries:
            return {}

        # 1. Dense Embeddings (å°è¯•ä½¿ç”¨ batch æ¥å£)
        try:
            dense_vecs = self.retriever.dense_embedding_func.embed_documents(queries)
        except AttributeError:  # å›é€€åˆ°å¾ªç¯
            dense_vecs = [self.retriever.dense_embedding_func.embed_query(q) for q in queries]

        # 2. Sparse Embeddings (é€‚é… BGE-M3)
        try:
            sparse_result = self.retriever.sparse_embedding_func.encode_queries(queries)["sparse"]
            sparse_vecs = csr_to_milvus_format(sparse_result)
        except Exception as e:
            self.logger.error(f"Batch sparse embedding failed: {e}")
            raise e

        # 3. Construct Cache
        cache: Dict[str, VectorSchema] = {}
        for i, query in enumerate(queries):
            cache[query]: VectorSchema = {"dense": dense_vecs[i], "sparse": sparse_vecs[i],}
        return cache

    def _get_source_priority(self, document: Document) -> int:
        return self.priority_map.get(document.metadata.get("source_type"), 0)

    def _upsert_doc(self, buffer: Dict[str, Document], doc: Document) -> None:
        """åˆå¹¶æ–‡æ¡£åˆ° Bufferï¼Œä¿ç•™é«˜ä¼˜å…ˆçº§ç‰ˆæœ¬"""
        if not (pk := doc.metadata.get("pk")):
            return

        if pk not in buffer:
            buffer[pk] = doc  # æ–°æ–‡æ¡£ï¼Œç›´æ¥åŠ å…¥
        else:
            # å·²å­˜åœ¨çš„æ–‡æ¡£ï¼Œæ£€æŸ¥ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆçº§æ›´é«˜ï¼Œè¦†ç›–æ—§æ–‡æ¡£ï¼Œä¿ç•™æ›´é‡è¦çš„ source_desc
            # ç›¸åŒä¼˜å…ˆçº§é»˜è®¤ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œé€šå¸¸ç¬¬ä¸€ä¸ª Query åœ¨ Planner ä¸­æ›´é‡è¦ã€‚
            old_prio = self._get_source_priority(buffer[pk])
            new_prio = self._get_source_priority(doc)
            if new_prio > old_prio:
                buffer[pk] = doc

    def _execute_static_retrieval(self, queries: List[str], embedding_cache: Dict) -> Dict[str, Document]:
        """
        è½¨é“ä¸€ï¼šé™æ€çŸ¥è¯†æ£€ç´¢
        - ä½¿ç”¨ Plan ç”Ÿæˆçš„æ³›åŒ– Query
        - ç›®æ ‡ï¼šstatic_knowledge åˆ†åŒº
        - åŒ…å«ï¼šTopology Expansion (çˆ¶èŠ‚ç‚¹/å…„å¼ŸèŠ‚ç‚¹/å†…éƒ¨é“¾æ¥)
        """
        # ä½¿ç”¨ Dict[pk, Document] ä»£æ›¿ List è¿›è¡Œå»é‡å’Œç®¡ç†
        candidate_buffer: Dict[str, Document] = {}
        print(f"   ğŸ“š [Static Track]: Processing {len(queries)} queries on 'static_knowledge'...")
        # éå†æ¯ä¸ª Query (Retrieval + Expansion)
        for query in queries:
            if not (vectors := embedding_cache.get(query)):
                continue
            try:
                # 1. Hybrid Search (è·å– Anchors)
                anchors = self.retriever.search_with_vectors(
                    dense_vec=vectors["dense"],
                    sparse_vec=vectors["sparse"],
                    partition_names=[STATIC_PARTITION_NAME]
                )
                # æ ‡è®°æ¥æº
                for doc in anchors:
                    doc.metadata["source_type"] = SourceType.ANCHOR
                    doc.metadata["source_desc"] = f"Direct hit by query: '{query}'"

                # 2. æ‹“æ‰‘æ‰©å±• (Graph Topology Expansion)
                expanded_docs = self.traverser.expand(anchors, vectors['dense'])
                current_batch = anchors + expanded_docs

                # 3. åŸºäºä¼˜å…ˆçº§çš„ Upsert (åˆå¹¶åˆ° Buffer)
                for doc in current_batch:
                    self._upsert_doc(candidate_buffer, doc)

                print(f"   Query: '{query}' -> Found {len(current_batch)} docs "
                      f"(Anchors: {len(anchors)}, Expanded: {len(expanded_docs)})")

            except Exception as e:
                print(f"âŒ Static retrieval error for query '{query}': {e}")
                continue  # å•ä¸ª query å¤±è´¥ä¸åº”é˜»æ–­æ•´ä¸ªæµç¨‹
        return candidate_buffer

    def _execute_dynamic_retrieval(self, technical_summary: str, embedding_cache: Dict) -> Dict[str, Document]:
        """
        è½¨é“äºŒï¼šåŠ¨æ€äº‹ä»¶æ£€ç´¢
        - ä½¿ç”¨ Analysis ä¸­çš„ Technical Summary (åŒ…å«å…·ä½“å®ä½“)
        - ç›®æ ‡ï¼šdynamic_events åˆ†åŒº
        - åŒ…å«ï¼šåŠ¨é™å…³è” (é€šè¿‡ related_links æ‹‰å–é™æ€æ‰‹å†Œ)
        """
        candidate_buffer: Dict[str, Document] = {}
        # è·å–å‘é‡
        if not (vectors := embedding_cache.get(technical_summary)):
            return {}

        print(f"   ğŸš¨ [Dynamic Track]: Searching 'dynamic_events' with summary...")

        try:
            # 1. åŠ¨æ€æ£€ç´¢ (Top-K è¾ƒå°ï¼Œä¾‹å¦‚ 2)
            # éœ€è¦retrieveræ”¯æŒåŠ¨æ€ä¼ å‚
            dynamic_hits = self.retriever.search_with_vectors(
                dense_vec=vectors["dense"],
                sparse_vec=vectors["sparse"],
                limit=2,
                partition_names=[DYNAMIC_PARTITION_NAME]  # æ˜¾å¼æŒ‡å®šåŠ¨æ€åˆ†åŒº
            )

            for doc in dynamic_hits:
                # æ ‡è®° Dynamic
                doc.metadata["source_type"] = SourceType.DYNAMIC
                doc.metadata["source_desc"] = "Runtime Event Match"
                self._upsert_doc(candidate_buffer, doc)

                # 2. åŠ¨é™å…³è” (Reverse Instantiation / Alignment)
                # æ£€æŸ¥åŠ¨æ€èŠ‚ç‚¹æ˜¯å¦é€šè¿‡ related_links æŒ‡å‘äº†é™æ€é”šç‚¹ï¼Œè¿™äº›é“¾æ¥æ˜¯åœ¨ RuntimeBridge å…¥åº“æ—¶è®¡ç®—å¥½çš„
                related_links = doc.metadata.get("related_links", [])

                static_anchor_pks = []
                for link in related_links:
                    if link.get("type") == "static_anchor":
                        static_anchor_pks.append(link.get("pk"))

                if static_anchor_pks:
                    print(f"      ğŸ”— Linked to {len(static_anchor_pks)} static anchors.")
                    # æ‰¹é‡æ‹‰å–è¿™äº›é™æ€æ–‡æ¡£ (å¤ç”¨ traverser çš„ batch_fetch)
                    linked_static_docs = self.traverser.batch_fetch(static_anchor_pks)

                    for static_doc in linked_static_docs:
                        static_doc.metadata["source_type"] = SourceType.LINK    # æˆ–è€…å« alignment_anchor
                        static_doc.metadata["source_desc"] = f"Aligned from Event: {doc.metadata.get('title')}"
                        self._upsert_doc(candidate_buffer, static_doc)

        except Exception as e:
            print(f"âŒ Dynamic retrieval error: {e}")

        return candidate_buffer

    def __call__(self, state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ£€ç´¢é€»è¾‘
        """
        # 1. è·å–ä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„åˆ†æç»“æœï¼Œå¹¶å®‰å…¨æ£€æŸ¥
        plan: ExecutionPlan = state.get("plan")
        analysis = state.get("analysis")
        if not plan or not plan.search_queries:
            print("âŒ No search queries found in state.")
            return {"retrieved_docs": [], "error": "No queries in plan"}

        # è·å–å½“å‰æ¬¡æ•° (é»˜è®¤ä¸º0)
        current_attempts = state.get("retrieval_attempts", 0)
        print(f"   ğŸ”„ Retrieval Attempts: {current_attempts + 1}")

        # 1. å‡†å¤‡ Query åˆ—è¡¨
        # é™æ€è½¨ Query
        static_queries = plan.search_queries
        # åŠ¨æ€è½¨ Query (ä»…å½“éœ€è¦è¯Šæ–­/æŸ¥è¯¢èµ„æºæ—¶ï¼Œä½¿ç”¨ technical_summaryï¼Œå› ä¸ºå®ƒä¿ç•™äº†å®ä½“ä¿¡æ¯)
        dynamic_queries = [analysis.technical_summary] if analysis and analysis.target_operation in self.dynamic_track_ops else []

        # 2. ç»Ÿä¸€ Embedding (Batch å¤„ç†æé«˜æ•ˆç‡)
        all_queries = static_queries + dynamic_queries
        print(f"ğŸ” Embedding {len(all_queries)} queries...")
        embedding_cache = self._batch_embed_queries(all_queries)

        # 3. å¹¶è¡Œ/ä¸²è¡Œæ‰§è¡ŒåŒè½¨æ£€ç´¢
        # A. é™æ€è½¨
        static_results = self._execute_static_retrieval(static_queries, embedding_cache)
        # B. åŠ¨æ€è½¨
        dynamic_results = self._execute_dynamic_retrieval(dynamic_queries[0], embedding_cache) if dynamic_queries else {}

        # 4. åˆå¹¶ç»“æœ
        # ç”±äº priority_map ä¸­ dynamic_event ä¼˜å…ˆçº§æœ€é«˜ï¼Œæ‰€ä»¥åŠ¨æ€äº‹ä»¶è‚¯å®šä¼šè¢«ä¿ç•™ã€‚
        final_buffer = static_results.copy()
        for doc in dynamic_results.values():
            self._upsert_doc(final_buffer, doc)

        all_candidates = list(final_buffer.values())
        print(f"âˆ‘ Total unique candidates after merging: {len(all_candidates)}")

        # 5. Rerank é˜¶æ®µ
        # Rerank éœ€è¦çŸ¥é“ retrieved_docsã€technical summaryã€last_message
        state_for_rerank = {
            "retrieved_docs": all_candidates,
            "analysis": state.get("analysis"),
            "message": state.get("messages")
        }

        # è°ƒç”¨ Rerank (å‡è®¾ RerankNode å·²ç»æ˜¯ä¸€ä¸ª callable)
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œå‡è®¾æˆ‘ä»¬å¯ä»¥ç›´æ¥å¤ç”¨ reranker å®ä¾‹çš„æ–¹æ³•ï¼Œæˆ–è€…åœ¨è¿™é‡Œç›´æ¥å®ä¾‹åŒ– RerankNode å¹¶è°ƒç”¨
        reranked_result = self.reranker(state_for_rerank, config=config)
        final_docs = reranked_result.get("retrieved_docs", [])
        print(f"   Found {len(final_docs)} relevant docs.")

        return {
            "retrieved_docs": final_docs,
            "tool_output": None,
        }  # æ¸…ç©ºä¹‹å‰çš„å·¥å…·è¾“å‡º