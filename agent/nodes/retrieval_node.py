import logging
from typing import List, Dict, Any, TypedDict, Optional

from langchain_core.documents import Document
from langchain_core.runnables import RunnableConfig

from agent.nodes import RerankNode
from agent.schemas import ExecutionPlan
from agent.state import AgentState
from retriever import MilvusHybridRetriever, GraphTraverser
from utils import csr_to_milvus_format


class VectorSchema(TypedDict):
    dense: Optional[List[float]]
    sparse: Optional[Dict[int, float]]

class RetrievalNode:
    """
    æ£€ç´¢èŠ‚ç‚¹ï¼Œæ‰¹å¤„ç†å‘é‡åµŒå…¥ï¼Œåˆ†ä¸‰é˜¶æ®µæ£€ç´¢ï¼šç²—ç­›ï¼ˆRetrievalï¼‰ã€æ‰©å±•ï¼ˆExpansionï¼‰å’Œç²¾ç­›ï¼ˆRerankï¼‰
    """
    logger = logging.getLogger(__name__)
    priority_map = {'anchor': 3, 'parent': 2, 'link': 1,  'sibling': 1}     # æ–‡æ¡£æ¥æºçš„ä¼˜å…ˆçº§

    def __init__(self, retriever: MilvusHybridRetriever, traverser: GraphTraverser, reranker: RerankNode):
        """
        åˆå§‹åŒ–æ£€ç´¢èŠ‚ç‚¹
        :param retriever: å·²åˆå§‹åŒ–çš„ MilvusHybridRetriever (æŒæœ‰ embedding models)
        :param traverser: å·²åˆå§‹åŒ–çš„ GraphTraverser (åªè´Ÿè´£æ‹“æ‰‘è®¡ç®—)
        :param reranker: å·²åˆå§‹åŒ–çš„ RerankNode
        """
        self.retriever = retriever
        self.traverser = traverser
        self.reranker = reranker

    def _batch_embed_queries(self, queries: List[str]) -> Dict[str, VectorSchema]:
        """
        Batch embed all queries using models from the retriever.
        """
        if not queries:
            return {}

        # 1. Dense Embeddings (å°è¯•ä½¿ç”¨ batch æ¥å£)
        try:
            dense_vecs = self.retriever.dense_embedding_func.embed_documents(queries)
        except AttributeError:  # å›é€€åˆ°å¾ªç¯
            dense_vecs = [self.retriever.dense_embedding_func.embed_query(q) for q in queries]

        # 2. Sparse Embeddings (é€‚é… BGE-M3)
        try:
            # å‡è®¾ sparse_embedding_func æ˜¯ BGE-M3 wrapperï¼Œå…·æœ‰ encode_queries
            sparse_result = self.retriever.sparse_embedding_func.encode_queries(queries)["sparse"]
            sparse_vecs = csr_to_milvus_format(sparse_result)

        except Exception as e:
            self.logger.error(f"Batch sparse embedding failed: {e}")
            raise e

        # 3. Construct Cache
        cache: Dict[str, VectorSchema] = {}
        for i, query in enumerate(queries):
            cache[query]: VectorSchema = {"dense": dense_vecs[i], "sparse": sparse_vecs[i],}

        return cache

    def _get_source_priority(self, document: Document) -> int:
        return self.priority_map.get(document.metadata.get("source_type", "unknown"), 0)

    def __call__(self, state: AgentState, config: RunnableConfig) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ£€ç´¢é€»è¾‘
        """
        # 1. è·å–ä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„åˆ†æç»“æœï¼Œå¹¶å®‰å…¨æ£€æŸ¥
        plan: ExecutionPlan = state.get("plan")
        if not plan or not plan.search_queries:
            print("âŒ No search queries found in state.")
            return {"retrieved_docs": [], "error": "No queries in plan"}

        # è·å–å½“å‰æ¬¡æ•° (é»˜è®¤ä¸º0)
        current_attempts = state.get("retrieval_attempts", 0)
        print(f"   ğŸ”„ Retrieval Attempts: {current_attempts + 1}")

        # æ‰¹é‡ Embeddingï¼Œç”Ÿæˆä¸Šä¸‹æ–‡ç¼“å­˜
        queries = plan.search_queries
        print(f"ğŸ” Processing {len(queries)} queries...")
        embedding_cache = self._batch_embed_queries(queries)

        # ä½¿ç”¨ Dict[pk, Document] ä»£æ›¿ List è¿›è¡Œå»é‡å’Œç®¡ç†
        # é”®æ˜¯ PKï¼Œå€¼æ˜¯ Document å¯¹è±¡
        candidate_buffer: Dict[str, Document] = {}

        # 2. éå†æ¯ä¸ª Query (Retrieval + Expansion)
        for query in plan.search_queries:
            vectors = embedding_cache.get(query)
            try:
                # è°ƒç”¨ MilvusHybridRetriever
                # A. Hybrid Search (è·å– Anchors)
                anchors = self.retriever.search_with_vectors(
                    dense_vec=vectors["dense"],
                    sparse_vec=vectors["sparse"],
                )
                # æ ‡è®°æ¥æº
                for doc in anchors:
                    doc.metadata["source_type"] = "anchor"
                    doc.metadata["source_desc"] = f"Direct hit by query: '{query}'"

                # B. Graph Expansion (ä¼ å…¥ Dense Vector å³å¯)
                expanded_docs = self.traverser.expand(anchors, vectors['dense'])
                current_batch = anchors + expanded_docs
                print(f"   Query: '{query}' -> Found {len(current_batch)} docs "
                      f"(Anchors: {len(anchors)}, Expanded: {len(expanded_docs)})")

                # C. åŸºäºä¼˜å…ˆçº§çš„ Upsert (åˆå¹¶åˆ° Buffer)
                for doc in current_batch:
                    if not (pk := doc.metadata.get("pk")):
                        continue

                    if pk not in candidate_buffer:
                        candidate_buffer[pk] = doc  # æ–°æ–‡æ¡£ï¼Œç›´æ¥åŠ å…¥
                    else:
                        # å·²å­˜åœ¨çš„æ–‡æ¡£ï¼Œæ£€æŸ¥ä¼˜å…ˆçº§
                        # ä¼˜å…ˆçº§æ›´é«˜ï¼Œè¦†ç›–æ—§æ–‡æ¡£ï¼Œä¿ç•™æ›´é‡è¦çš„ source_desc
                        # ç›¸åŒä¼˜å…ˆçº§é»˜è®¤ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œé€šå¸¸ç¬¬ä¸€ä¸ª Query åœ¨ Planner ä¸­æ›´é‡è¦ã€‚
                        old_prio = self._get_source_priority(candidate_buffer[pk])
                        new_prio = self._get_source_priority(doc)
                        if new_prio > old_prio:
                            candidate_buffer[pk] = doc

            except Exception as e:
                print(f"âŒ Error retrieving for query '{query}': {e}")
                continue    # å•ä¸ª query å¤±è´¥ä¸åº”é˜»æ–­æ•´ä¸ªæµç¨‹

        # å°† Buffer è½¬å› List
        all_candidates = list(candidate_buffer.values())
        print(f"âˆ‘ Total unique candidates after merging: {len(all_candidates)}")

        # 3. Rerank é˜¶æ®µ
        # Rerank éœ€è¦çŸ¥é“ retrieved_docsã€technical summaryã€last_message
        state_for_rerank = {
            "retrieved_docs": all_candidates,
            "analysis": state.get("analysis"),
            "message": state.get("messages")
        }

        # è°ƒç”¨ Rerank (å‡è®¾ RerankNode å·²ç»æ˜¯ä¸€ä¸ª callable)
        # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œå‡è®¾æˆ‘ä»¬å¯ä»¥ç›´æ¥å¤ç”¨ reranker å®ä¾‹çš„æ–¹æ³•
        # æˆ–è€…åœ¨è¿™é‡Œç›´æ¥å®ä¾‹åŒ– RerankNode å¹¶è°ƒç”¨
        reranked_result = self.reranker(state_for_rerank, config=config)
        final_docs = reranked_result.get("retrieved_docs", [])
        print(f"   Found {len(final_docs)} relevant docs.")

        return {
            "retrieved_docs": final_docs,
            "tool_output": None,
        }  # æ¸…ç©ºä¹‹å‰çš„å·¥å…·è¾“å‡º