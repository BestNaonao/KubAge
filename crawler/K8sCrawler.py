import logging
import os
from abc import ABC, abstractmethod
from typing import List, Dict, Set, Tuple
from urllib.parse import urljoin

from bs4 import BeautifulSoup

from utils.html2md_utils import convert_to_markdown, get_span_path

HEADERS: Dict[str, str] = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
    "Connection": "keep-alive"
}
NO_REF_URL: str = "https://kubernetes.io"
BASE_URL: str = "https://kubernetes.io/zh-cn/docs/"
START_URL: str = BASE_URL + "home/"
ELEMENT_NAME_LIST: List[str] = ['p', 'h1', 'h2', 'h3', 'h4', 'li', 'code']
EXCLUDE_HREF = ["/contribute", "/blog", "/training", "/careers", "/partners", "/community", "/test", "/feature-gates-removed"]
REMOVE_TEXT: str = "æ­¤é¡µæ˜¯å¦å¯¹ä½ æœ‰å¸®åŠ©"


class K8sCrawler(ABC):
    def __init__(self, num_workers: int, save_dir: str):
        self.num_workers = num_workers
        self.save_dir = save_dir
        self.found: Set[str] = set()
        self.errored: List[str] = []  # åªè¿½åŠ 
        self.filename_counter = {}
        self.headers: Dict[str, str] = HEADERS
        self.start_url = START_URL

        # æ—¥å¿—é…ç½®
        self.logger = logging.getLogger(type(self).__name__)
        self.logger.setLevel(logging.DEBUG)
        handler = logging.FileHandler("crawler_async.log", encoding='utf-8')
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        self.logger.addHandler(handler)

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

    def initialize(self):
        self.found.clear()
        self.errored.clear()
        self.found.add(START_URL)

    @staticmethod
    def extract_new_urls(soup, url) -> List[str]:
        """æå–é¡µé¢ä¸­çš„æ–°URLå¹¶è¿”å›žåˆ—è¡¨"""
        new_urls = []
        parent_elements = soup.select("ul.ul-1")
        for parent_element in parent_elements:
            for link in parent_element.find_all('a', href=True):
                href = link.get('href')
                full_url = urljoin(BASE_URL, link['href'])
                if any(prefix in href for prefix in EXCLUDE_HREF) or not full_url.startswith(BASE_URL):
                    continue
                if '#' in full_url or full_url == url:
                    continue
                new_urls.append(full_url)
        return new_urls

    def get_unique_filename(self, base_name: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…å†²çª"""
        # å¦‚æžœæ–‡ä»¶åå·²å­˜åœ¨ï¼Œæ·»åŠ è®¡æ•°å™¨åŽç¼€
        if base_name in self.filename_counter:
            self.filename_counter[base_name] += 1
            return f"{base_name}_{self.filename_counter[base_name]}"
        else:
            self.filename_counter[base_name] = 1
            return base_name

    def parse_html(self, html: str, url: str) -> Tuple[List[str], str, str]:
        soup = BeautifulSoup(html, 'lxml')
        new_urls = self.extract_new_urls(soup, url)
        span_name = get_span_path(soup, url, NO_REF_URL)
        markdown_content = convert_to_markdown(soup)
        return new_urls, span_name, markdown_content

    def save(self, markdown_content: str, url: str, doc_name: str):
        """æå–ç½‘é¡µä¸­çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œæ­£æ–‡ï¼Œæœ€åŽä¿å­˜"""
        if not markdown_content:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹: {url}")
            return

        save_path = os.path.join(self.save_dir, f"{doc_name}.md")
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        self.logger.info(f"å·²ä¿å­˜: {save_path} (å­—ç¬¦æ•°: {len(markdown_content)})")

    def show_progress(self, done: int, total: int, active: int):
        if total > 0:
            progress = done / total * 100
            print(
                f"\rðŸ“Š è¿›åº¦: {done}/{total} ({progress:.1f}%) | é˜Ÿåˆ—: {total - done} | "
                f"æ´»è·ƒWorker: {active} / {self.num_workers} | é”™è¯¯: {len(self.errored)}", end='', flush=True
            )

    def quit_print(self):
        print("\n" + "=" * 50)
        self.logger.info(f"çˆ¬å–å®Œæˆ! æ€»å…±è®¿é—®: {len(self.found)} é¡µé¢, é”™è¯¯: {len(self.errored)}")

        if self.errored:
            self.logger.info("----- ä»¥ä¸‹URLèŽ·å–å¤±è´¥:")
            for url in self.errored:
                self.logger.info(f"--- {url}")

    @abstractmethod
    def run(self) -> None:
        raise NotImplementedError

