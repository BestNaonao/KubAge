import logging
import os
import re
from abc import ABC, abstractmethod
from typing import List, Dict, Set, Tuple
from urllib.parse import urljoin

from bs4 import BeautifulSoup

from utils.html2md_utils import convert_to_markdown, get_span_path

HEADERS: Dict[str, str] = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
    "Connection": "keep-alive"
}
NO_REF_URL: str = "https://kubernetes.io"
BASE_URL: str = "https://kubernetes.io/zh-cn/docs/"
START_URL: str = BASE_URL + "home/"
ELEMENT_NAME_LIST: List[str] = ['p', 'h1', 'h2', 'h3', 'h4', 'li', 'code']
EXCLUDE_HREF = ["/contribute", "/blog", "/training", "/careers", "/partners", "/community", "/test",
                "/feature-gates-removed", "/reference/issues-security"]
REMOVE_TEXT: str = "æ­¤é¡µæ˜¯å¦å¯¹ä½ æœ‰å¸®åŠ©"


class K8sCrawler(ABC):
    def __init__(self, num_workers: int, save_dir: str):
        self.num_workers = num_workers
        self.save_dir = save_dir
        self.found: Set[str] = set()
        self.errored: List[str] = []  # åªè¿½åŠ 
        self.filename_counter = {}
        self.headers: Dict[str, str] = HEADERS
        self.start_url = START_URL

        # æ—¥å¿—é…ç½®
        self.logger = logging.getLogger(type(self).__name__)
        self.logger.setLevel(logging.DEBUG)
        handler = logging.FileHandler("crawler_async.log", encoding='utf-8')
        handler.setLevel(logging.DEBUG)
        handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
        self.logger.addHandler(handler)

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

    def initialize(self):
        self.found.clear()
        self.errored.clear()
        self.found.add(START_URL)

    @staticmethod
    def extract_new_urls(soup, url) -> List[str]:
        """æå–é¡µé¢ä¸­çš„æ–°URLå¹¶è¿”å›åˆ—è¡¨"""
        new_urls = []
        parent_elements = soup.select("ul.ul-1")
        for parent_element in parent_elements:
            for link in parent_element.find_all('a', href=True):
                href = link.get('href')
                full_url = urljoin(BASE_URL, link['href'])
                if any(prefix in href for prefix in EXCLUDE_HREF) or not full_url.startswith(BASE_URL):
                    continue
                if '#' in full_url or full_url == url:
                    continue
                new_urls.append(full_url)
        return new_urls

    def get_unique_filename(self, base_name: str) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼Œé¿å…å†²çª"""
        # å¦‚æœæ–‡ä»¶åå·²å­˜åœ¨ï¼Œæ·»åŠ è®¡æ•°å™¨åç¼€
        if base_name in self.filename_counter:
            self.filename_counter[base_name] += 1
            return f"{base_name}_{self.filename_counter[base_name]}"
        else:
            self.filename_counter[base_name] = 1
            return base_name

    def parse_html(self, html: str, url: str) -> Tuple[List[str], str, str]:
        soup = BeautifulSoup(html, 'lxml')
        new_urls = self.extract_new_urls(soup, url)
        span_name = get_span_path(soup, url, NO_REF_URL)
        markdown_content = convert_to_markdown(soup)
        return new_urls, span_name, markdown_content

    def save(self, markdown_content: str, url: str, doc_name: str):
        """æå–ç½‘é¡µä¸­çš„å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œæ­£æ–‡ï¼Œæœ€åä¿å­˜"""
        if not markdown_content:
            self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹: {url}")
            return

        save_path = os.path.join(self.save_dir, f"{doc_name}.md")
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        self.logger.info(f"å·²ä¿å­˜: {save_path} (å­—ç¬¦æ•°: {len(markdown_content)})")

    def show_progress(self, done: int, total: int, active: int):
        if total > 0:
            progress = done / total * 100
            print(
                f"\rğŸ“Š è¿›åº¦: {done}/{total} ({progress:.1f}%) | é˜Ÿåˆ—: {total - done} | "
                f"æ´»è·ƒWorker: {active} / {self.num_workers} | é”™è¯¯: {len(self.errored)}", end='', flush=True
            )

    def quit_print(self):
        print("\n" + "=" * 50)
        self.logger.info(f"çˆ¬å–å®Œæˆ! æ€»å…±è®¿é—®: {len(self.found)} é¡µé¢, é”™è¯¯: {len(self.errored)}")

        if self.errored:
            self.logger.info("----- ä»¥ä¸‹URLè·å–å¤±è´¥:")
            for url in self.errored:
                self.logger.info(f"--- {url}")

    @abstractmethod
    def run(self) -> None:
        raise NotImplementedError

    def insert_title_at_regex_position(self, file_name: str, title: str, regex_pattern: str):
        """
        åœ¨æ–‡ä»¶ä¸­åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼çš„ä½ç½®æ’å…¥æ ‡é¢˜

        å‚æ•°:
        file_path (str): æ–‡ä»¶è·¯å¾„
        title (str): è¦æ’å…¥çš„æ ‡é¢˜ï¼ˆå¸¦æ ¼å¼ï¼Œå¦‚"###### CEL è¡¨è¾¾å¼è§„åˆ™"ï¼‰
        regex_pattern (str): ç”¨äºæœç´¢ä½ç½®çš„æ­£åˆ™è¡¨è¾¾å¼
        """
        try:
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_path = os.path.join(self.save_dir, file_name)
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()

            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æœç´¢åŒ¹é…ä½ç½®
            match = re.search(regex_pattern, content, re.MULTILINE)

            if match:
                # è·å–åŒ¹é…ä½ç½®
                start_pos = match.start()

                # åœ¨åŒ¹é…ä½ç½®å‰æ’å…¥æ ‡é¢˜ï¼ˆå¸¦æ¢è¡Œç¬¦ï¼‰
                new_content = content[:start_pos] + f"\n{title}\n" + content[start_pos:]

                # å†™å›æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as file:
                    file.write(new_content)

                print(f"æˆåŠŸåœ¨æ–‡ä»¶ä¸­æ’å…¥æ ‡é¢˜: {title}")
                print(f"æ’å…¥ä½ç½®: ç¬¬{content[:start_pos].count('\n') + 1}è¡Œé™„è¿‘")
                return True
            else:
                print("æœªæ‰¾åˆ°åŒ¹é…çš„ä½ç½®")
                return False

        except FileNotFoundError:
            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
            return False
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return False

