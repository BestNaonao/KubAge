import re
from collections import defaultdict
import os
import argparse
import numpy as np
from docx import Document


def check_docx_files(folder_path, target_text):
    """
    æ£€æŸ¥æ–‡ä»¶å¤¹ä¸­çš„docxæ–‡ä»¶æ˜¯å¦åŒ…å«ç›®æ ‡æ–‡æœ¬å¹¶ç»Ÿè®¡åç»­æ–‡æœ¬é•¿åº¦
    :param folder_path: docxæ–‡ä»¶æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„
    :param target_text: è¦æŸ¥æ‰¾çš„ç›®æ ‡æ–‡æœ¬
    :return: åŒ…å«åŒ¹é…ä¿¡æ¯å’Œç»Ÿè®¡ç»“æœçš„æ•°æ®
    """
    # åˆå§‹åŒ–ç»“æœå­˜å‚¨
    results = []
    total_files = 0
    matched_files = 0
    post_lengths = []  # å­˜å‚¨æ¯ä¸ªåŒ¹é…é¡¹åç»­æ–‡æœ¬çš„é•¿åº¦

    # éå†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for filename in os.listdir(folder_path):
        if filename.endswith('.md'):
            file_path = os.path.join(folder_path, filename)
            total_files += 1

            try:
                doc = Document(file_path)
                full_text = '\n'.join([para.text for para in doc.paragraphs])

                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡æ–‡æœ¬
                if target_text in full_text:
                    matched_files += 1

                    # æŸ¥æ‰¾æ‰€æœ‰ç›®æ ‡æ–‡æœ¬ä½ç½®
                    start_indices = [m.start() for m in re.finditer(re.escape(target_text), full_text)]

                    for start_idx in start_indices:
                        # è®¡ç®—ç›®æ ‡æ–‡æœ¬åçš„å†…å®¹èµ·å§‹ä½ç½®
                        post_start = start_idx + len(target_text)
                        # æå–ç›®æ ‡æ–‡æœ¬åçš„å…¨éƒ¨å†…å®¹
                        post_text = full_text[post_start:]

                        # æ¸…ç†å¤šä½™æ¢è¡Œç¬¦
                        cleaned_text = re.sub(r'\n{3,}', '\n\n', post_text)

                        # è®°å½•åç»­æ–‡æœ¬é•¿åº¦
                        post_length = len(post_text)
                        post_lengths.append(post_length)

                        # æ·»åŠ åˆ°ç»“æœ
                        results.append({
                            'filename': filename,
                            'position': start_idx,
                            'post_text': cleaned_text,
                            'post_length': post_length
                        })
                else:
                    print("xxx: " + filename)
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {str(e)}")
                results.append({
                    'filename': filename,
                    'error': str(e)
                })

    # è®¡ç®—æ¯”ä¾‹
    if total_files > 0:
        percentage = (matched_files / total_files) * 100
    else:
        percentage = 0

    # è®¡ç®—åç»­æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
    length_stats = {}
    if post_lengths:
        length_stats = {
            'max': max(post_lengths),
            'min': min(post_lengths),
            'mean': np.mean(post_lengths),
            'median': np.median(post_lengths),
            'total': sum(post_lengths),
            'count': len(post_lengths)
        }

    return {
        'matches': results,
        'stats': {
            'total_files': total_files,
            'matched_files': matched_files,
            'percentage': percentage,
            'post_length_stats': length_stats
        }
    }


def format_length(length):
    """æ ¼å¼åŒ–é•¿åº¦å€¼ä¸ºæ˜“è¯»å½¢å¼"""
    if length < 1000:
        return f"{length} å­—ç¬¦"
    elif length < 1000000:
        return f"{length / 1000:.1f}åƒå­—ç¬¦"
    else:
        return f"{length / 1000000:.2f}å…†å­—ç¬¦"


def report_content_check(results):
    """ç”Ÿæˆæ–‡æ¡£å†…å®¹æ£€æŸ¥æŠ¥å‘Š"""
    report_lines = []
    report_lines.append("=" * 50)
    report_lines.append("æ–‡æ¡£å†…å®¹æ£€æŸ¥æŠ¥å‘Š")
    report_lines.append("=" * 50)

    # æ‰“å°åŒ¹é…å†…å®¹å’Œä½ç½®
    for match in results['matches']:
        if 'post_text' in match:
            report_lines.append(f"\nğŸ“„ æ–‡ä»¶: {match['filename']}")
            report_lines.append(f"ğŸ“ ä½ç½®: {match['position']}")
            report_lines.append(f"ğŸ“ åç»­æ–‡æœ¬é•¿åº¦: {format_length(match['post_length'])}")

            # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
            preview = match['post_text'][:200] + ("..." if len(match['post_text']) > 200 else "")
            report_lines.append(f"ğŸ“ åç»­å†…å®¹é¢„è§ˆ: \n{preview}")
            report_lines.append('-' * 50)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = results['stats']
    report_lines.append("\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
    report_lines.append(f"æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
    report_lines.append(f"åŒ…å«ç›®æ ‡æ–‡æœ¬çš„æ–‡ä»¶æ•°: {stats['matched_files']}")
    report_lines.append(f"å æ¯”: {stats['percentage']:.2f}%")

    # æ‰“å°åç»­æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
    if stats['post_length_stats']:
        len_stats = stats['post_length_stats']
        report_lines.append("\nğŸ“ åç»­æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:")
        report_lines.append(f"åŒ¹é…é¡¹æ•°é‡: {len_stats['count']}")
        report_lines.append(f"æœ€å¤§é•¿åº¦: {format_length(len_stats['max'])}")
        report_lines.append(f"æœ€å°é•¿åº¦: {format_length(len_stats['min'])}")
        report_lines.append(f"å¹³å‡é•¿åº¦: {format_length(len_stats['mean'])}")
        report_lines.append(f"ä¸­ä½é•¿åº¦: {format_length(len_stats['median'])}")
        report_lines.append(f"æ€»é•¿åº¦: {format_length(len_stats['total'])}")
    else:
        report_lines.append("\nâš ï¸ æœªæ‰¾åˆ°åŒ¹é…é¡¹ï¼Œæ— æ³•è®¡ç®—åç»­æ–‡æœ¬é•¿åº¦ç»Ÿè®¡")

    return "\n".join(report_lines)


# ======================== æ—¥å¿—åˆ†æåŠŸèƒ½ ========================
def analyze_crawler_log(log_lines, save_dir=None):
    """åˆ†æçˆ¬è™«æ—¥å¿—æ–‡ä»¶ï¼Œæå–è®¿é—®çš„URLã€ä¿å­˜çš„æ–‡ä»¶ã€å¤±è´¥ä¿¡æ¯ç­‰"""
    # å¦‚æœä¼ å…¥çš„æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹
    if isinstance(log_lines, str) and os.path.isfile(log_lines):
        with open(log_lines, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()

    # è§£ææ—¥å¿—çš„æ­£åˆ™è¡¨è¾¾å¼
    start_pattern = re.compile(r'å¼€å§‹çˆ¬å–: (https?://\S+)')
    saved_pattern = re.compile(r'æ–‡æ¡£å·²ä¿å­˜ä¸º (.+\.docx)')
    failed_pattern = re.compile(r'âŒ å¤„ç†å¤±è´¥ (https?://\S+):')
    progress_pattern = re.compile(r'ğŸ“Š è¿›åº¦: (\d+)/(\d+)')
    thread_pattern = re.compile(r'çº¿ç¨‹: (Worker-\d+)')

    # å­˜å‚¨ç»“æœçš„æ•°æ®ç»“æ„
    results = {
        'visited_urls': set(),
        'saved_files': set(),
        'failed_urls': set(),
        'url_to_file': {},
        'file_to_urls': defaultdict(list),
        'thread_activities': defaultdict(list),
        'max_visited': 0,
        'max_total': 0
    }

    current_thread = None

    for line in log_lines:
        # æ•è·å½“å‰çº¿ç¨‹
        thread_match = thread_pattern.search(line)
        if thread_match:
            current_thread = thread_match.group(1)

        # æ•è·å¼€å§‹çˆ¬å–çš„URL
        start_match = start_pattern.search(line)
        if start_match:
            url = start_match.group(1)
            results['visited_urls'].add(url)
            if current_thread:
                results['thread_activities'][current_thread].append(('start', url))

        # æ•è·ä¿å­˜çš„æ–‡ä»¶
        saved_match = saved_pattern.search(line)
        if saved_match:
            filepath = saved_match.group(1)
            filename = os.path.basename(filepath)
            results['saved_files'].add(filename)
            if current_thread:
                results['thread_activities'][current_thread].append(('saved', filename))

        # æ•è·å¤±è´¥çš„URL
        failed_match = failed_pattern.search(line)
        if failed_match:
            url = failed_match.group(1)
            results['failed_urls'].add(url)
            if current_thread:
                results['thread_activities'][current_thread].append(('failed', url))

        # æ•è·è¿›åº¦ä¿¡æ¯
        progress_match = progress_pattern.search(line)
        if progress_match:
            visited = int(progress_match.group(1))
            total = int(progress_match.group(2))
            if total > results['max_total']:
                results['max_total'] = total
                results['max_visited'] = visited

    # æ„å»ºURLå’Œæ–‡ä»¶çš„æ˜ å°„å…³ç³»
    for thread, activities in results['thread_activities'].items():
        current_url = None
        for action, value in activities:
            if action == 'start':
                current_url = value
            elif action == 'saved' and current_url:
                results['url_to_file'][current_url] = value
                results['file_to_urls'][value].append(current_url)

    # åˆ†ææ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…æ–‡ä»¶ï¼ˆå¦‚æœæä¾›äº†ä¿å­˜ç›®å½•ï¼‰
    actual_files = set()
    if save_dir and os.path.isdir(save_dir):
        actual_files = set(os.listdir(save_dir))
        actual_files = {f for f in actual_files if f.endswith('.docx')}

    return results, actual_files


def report_crawler_findings(results, actual_files=None):
    """ç”Ÿæˆå¹¶æ‰“å°çˆ¬è™«è¿‡ç¨‹åˆ†ææŠ¥å‘Š"""
    report = []
    report.append("=" * 50)
    report.append("çˆ¬è™«æ—¥å¿—åˆ†ææŠ¥å‘Š")
    report.append("=" * 50)

    # 1. åŸºæœ¬ç»Ÿè®¡
    report.append("\n[åŸºæœ¬ç»Ÿè®¡]")
    report.append(f"æ—¥å¿—ä¸­è®°å½•çš„è®¿é—®URLæ•°é‡: {len(results['visited_urls'])}")
    report.append(f"æ—¥å¿—ä¸­è®°å½•çš„ä¿å­˜æ–‡ä»¶æ•°é‡: {len(results['saved_files'])}")
    report.append(f"æ—¥å¿—ä¸­è®°å½•çš„å¤±è´¥URLæ•°é‡: {len(results['failed_urls'])}")

    if actual_files is not None:
        report.append(f"æ–‡ä»¶ç³»ç»Ÿä¸­çš„å®é™…æ–‡ä»¶æ•°é‡: {len(actual_files)}")
    report.append(f"æœ€ç»ˆè¿›åº¦: {results['max_visited']}/{results['max_total']}")

    # 2. ç¼ºå¤±æ–‡ä»¶åˆ†æï¼ˆå¦‚æœæœ‰å®é™…æ–‡ä»¶ä¿¡æ¯ï¼‰
    if actual_files is not None:
        missing_in_log = actual_files - results['saved_files']
        missing_in_fs = results['saved_files'] - actual_files

        report.append("\n[æ–‡ä»¶ç³»ç»Ÿå·®å¼‚]")
        if missing_in_log:
            report.append(f"è­¦å‘Š: {len(missing_in_log)}ä¸ªæ–‡ä»¶å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿä½†æœªåœ¨æ—¥å¿—ä¸­è®°å½•")
            report.append("è¿™äº›æ–‡ä»¶å¯èƒ½æ˜¯ä¹‹å‰è¿è¡Œç•™ä¸‹çš„æˆ–æ‰‹åŠ¨æ·»åŠ çš„")

        if missing_in_fs:
            report.append(f"ä¸¥é‡: {len(missing_in_fs)}ä¸ªæ–‡ä»¶åœ¨æ—¥å¿—ä¸­è®°å½•ä½†ä¸å­˜åœ¨äºæ–‡ä»¶ç³»ç»Ÿ")
            for file in missing_in_fs:
                report.append(f"  - {file}")

    # 3. å¤±è´¥URLåˆ†æ
    if results['failed_urls']:
        report.append("\n[å¤±è´¥URLåˆ—è¡¨]")
        for url in results['failed_urls']:
            report.append(f"  - {url}")

    # 4. æ–‡ä»¶å†²çªåˆ†æ
    conflict_files = {f: urls for f, urls in results['file_to_urls'].items() if len(urls) > 1}

    if conflict_files:
        report.append("\n[æ–‡ä»¶åå†²çªè­¦å‘Š]")
        report.append(f"å‘ç° {len(conflict_files)} ä¸ªæ–‡ä»¶åè¢«å¤šä¸ªURLä½¿ç”¨:")
        for filename, urls in conflict_files.items():
            report.append(f"\næ–‡ä»¶å: {filename}")
            report.append("å¯¹åº”çš„URL:")
            for url in urls:
                report.append(f"  - {url}")
    else:
        report.append("\n[æ–‡ä»¶åå†²çªæ£€æŸ¥] æœªå‘ç°æ–‡ä»¶åå†²çª")

    # 5. æœªä¿å­˜URLåˆ†æ
    unsaved_urls = results['visited_urls'] - set(results['url_to_file'].keys()) - results['failed_urls']

    if unsaved_urls:
        report.append("\n[æœªä¿å­˜URLåˆ†æ]")
        report.append(f"å‘ç° {len(unsaved_urls)} ä¸ªURLè¢«è®¿é—®ä½†æœªä¿å­˜:")
        for url in unsaved_urls:
            report.append(f"  - {url}")

        # å°è¯•æ‰¾å‡ºæœ€åè®¿é—®çš„URL
        report.append("\nå¯èƒ½çš„ç½ªé­ç¥¸é¦–(æœ€åè®¿é—®çš„URL):")
        last_url = list(unsaved_urls)[-1] if unsaved_urls else None
        report.append(f"  - {last_url}")
    else:
        report.append("\n[æœªä¿å­˜URLåˆ†æ] æ‰€æœ‰è®¿é—®çš„URLéƒ½å·²ä¿å­˜æˆ–æ ‡è®°ä¸ºå¤±è´¥")

    return "\n".join(report)


def analyze_crawler_errors(log_lines):
    """åˆ†æçˆ¬è™«æ—¥å¿—æ–‡ä»¶ä¸­çš„é”™è¯¯"""
    # å¦‚æœä¼ å…¥çš„æ˜¯æ–‡ä»¶è·¯å¾„ï¼Œåˆ™è¯»å–æ–‡ä»¶å†…å®¹
    if isinstance(log_lines, str) and os.path.isfile(log_lines):
        with open(log_lines, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()

    # é”™è¯¯ç»Ÿè®¡å­—å…¸ç»“æ„: {é”™è¯¯ç±»å‹: {"count": æ•°é‡, "urls": [URLåˆ—è¡¨]}}
    error_stats = defaultdict(lambda: {"count": 0, "urls": set()})

    # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æé«˜æ•ˆç‡
    error_pattern = re.compile(r"âŒ å¤„ç†å¤±è´¥ (.*?): (.*)")
    url_extract_pattern = re.compile(r"https?://\S+")

    for line in log_lines:
        # æ£€æŸ¥æ˜¯å¦ä¸ºé”™è¯¯è¡Œ
        match = error_pattern.search(line)
        if not match:
            continue

        # æå–URLå’Œé”™è¯¯ä¿¡æ¯
        raw_url, error_msg = match.groups()

        # æ¸…ç†URLï¼ˆç§»é™¤å¯èƒ½çš„å°¾éšæ ‡ç‚¹ï¼‰
        clean_url = raw_url.strip()
        if clean_url.endswith(('.', ',')):
            clean_url = clean_url[:-1]

        # æ ‡å‡†åŒ–é”™è¯¯ä¿¡æ¯
        normalized_error = error_msg.strip()
        if 'HTTPSConnectionPool' in normalized_error:
            normalized_error = "ConnectionError"
        elif 'Read timed out' in normalized_error:
            normalized_error = "TimeoutError"
        elif '404' in normalized_error:
            normalized_error = "HTTP 404"
        elif 'SSLError' in normalized_error:
            normalized_error = "SSLError"

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        error_stats[normalized_error]["count"] += 1
        error_stats[normalized_error]["urls"].add(clean_url)

    return dict(error_stats)


def save_error_report(stats, output_file):
    """ä¿å­˜é”™è¯¯ç»Ÿè®¡æŠ¥å‘Šåˆ°æ–‡ä»¶"""
    report_lines = []
    report_lines.append("çˆ¬è™«é”™è¯¯ç»Ÿè®¡åˆ†ææŠ¥å‘Š")
    report_lines.append("=" * 50)
    report_lines.append("")

    # æŒ‰é”™è¯¯æ•°é‡é™åºæ’åº
    sorted_errors = sorted(stats.items(), key=lambda x: x[1]["count"], reverse=True)

    for error_type, data in sorted_errors:
        report_lines.append(f"é”™è¯¯ç±»å‹: {error_type}")
        report_lines.append(f"å‡ºç°æ¬¡æ•°: {data['count']}")
        report_lines.append(f"ç›¸å…³URL ({len(data['urls'])}ä¸ª):")

        # åˆ—å‡ºç›¸å…³URL
        for url in data['urls']:
            report_lines.append(f"  - {url}")

        report_lines.append("")
        report_lines.append("-" * 50)
        report_lines.append("")

    with open(output_file, 'w', encoding='utf-8') as report:
        report.write("\n".join(report_lines))


def generate_error_summary(stats):
    """ç”Ÿæˆé”™è¯¯æ‘˜è¦ä¿¡æ¯"""
    if not stats:
        return "æœªå‘ç°é”™è¯¯ä¿¡æ¯"

    summary = []
    summary.append("é”™è¯¯æ‘˜è¦:")
    summary.append(f"{'é”™è¯¯ç±»å‹':<25} | {'æ¬¡æ•°':<5} | {'å½±å“URLæ•°é‡':<10}")
    summary.append("-" * 50)

    for error, data in sorted(stats.items(), key=lambda x: x[1]["count"], reverse=True):
        summary.append(f"{error:<25} | {data['count']:<5} | {len(data['urls']):<10}")

    return "\n".join(summary)


# ======================== ä¸»ç¨‹åº ========================
def main():
    parser = argparse.ArgumentParser(description='ç»¼åˆæ—¥å¿—åˆ†æå’Œæ–‡æ¡£æ£€æŸ¥å·¥å…·')
    parser.add_argument('mode', nargs='?', choices=['log', 'content', 'errors'],
                        help='åˆ†ææ¨¡å¼: log(æ—¥å¿—è¿‡ç¨‹åˆ†æ), content(æ–‡æ¡£å†…å®¹æ£€æŸ¥), errors(é”™è¯¯åˆ†æ)')

    # æ—¥å¿—åˆ†æå‚æ•°
    parser.add_argument('--log-file', help='çˆ¬è™«æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--doc-dir', help='æ–‡æ¡£ä¿å­˜ç›®å½•ï¼ˆç”¨äºæ–‡ä»¶ç³»ç»ŸéªŒè¯ï¼‰', default='../raw_data')

    # æ–‡æ¡£å†…å®¹æ£€æŸ¥å‚æ•°
    parser.add_argument('--target-text', help='æ–‡æ¡£å†…å®¹æ£€æŸ¥çš„ç›®æ ‡æ–‡æœ¬', default='æ­¤é¡µæ˜¯å¦å¯¹ä½ æœ‰å¸®åŠ©ï¼Ÿ')

    # é”™è¯¯åˆ†æå‚æ•°
    parser.add_argument('--error-report', help='é”™è¯¯æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶', default='crawler_errors_report.txt')

    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--output', help='ç»“æœè¾“å‡ºæ–‡ä»¶', default=None)

    args = parser.parse_args()

    output_file = args.output

    # æ ¹æ®æ¨¡å¼æ‰§è¡Œä¸åŒçš„åˆ†æ
    if args.mode == 'log':
        if not args.log_file:
            print("é”™è¯¯: æ—¥å¿—åˆ†æéœ€è¦æŒ‡å®š--log-fileå‚æ•°")
            return

        print("\næ‰§è¡Œçˆ¬å–è¿‡ç¨‹åˆ†æ...")
        with open(args.log_file, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()

        results, actual_files = analyze_crawler_log(log_lines, args.doc_dir)
        report = report_crawler_findings(results, actual_files)

    elif args.mode == 'content':
        print("\næ‰§è¡Œæ–‡æ¡£å†…å®¹æ£€æŸ¥...")
        results = check_docx_files(args.doc_dir, args.target_text)
        report = report_content_check(results)

    elif args.mode == 'errors':
        if not args.log_file:
            print("é”™è¯¯: é”™è¯¯åˆ†æéœ€è¦æŒ‡å®š--log-fileå‚æ•°")
            return

        print("\næ‰§è¡Œé”™è¯¯åˆ†æ...")
        with open(args.log_file, 'r', encoding='utf-8') as f:
            log_lines = f.readlines()

        error_stats = analyze_crawler_errors(log_lines)
        if error_stats:
            save_error_report(error_stats, args.error_report)
            report = generate_error_summary(error_stats)
            report += f"\nè¯¦ç»†é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {args.error_report}"
        else:
            report = "æœªå‘ç°é”™è¯¯ä¿¡æ¯"

    else:
        print("è¯·æŒ‡å®šåˆ†ææ¨¡å¼: log, content æˆ– errors")
        return

    # è¾“å‡ºç»“æœ
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    else:
        print("\n" + report)


if __name__ == '__main__':
    """
    # æ–‡æ¡£å†…å®¹æ£€æŸ¥
    python crawler_analysis.py content --doc-dir ../raw_data --target-text "ç›®æ ‡æ–‡æœ¬"
    
    # æ—¥å¿—è¿‡ç¨‹åˆ†æ
    python crawler_analysis.py log --log-file crawler.log --doc-dir ../raw_data
    
    # é”™è¯¯åˆ†æ
    python crawler_analysis.py errors --log-file crawler.log
    
    # è¾“å‡ºç»“æœåˆ°æ–‡ä»¶
    python crawler_analysis.py content --doc-dir ../raw_data --output content_report.txt
    """
    main()